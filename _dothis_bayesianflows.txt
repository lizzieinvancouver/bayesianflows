Started 22 August 2023

I have feedback from co-authors I should add it (notes below) in and ...

(1) suppneedtofix file -- ASCII issues here that I should fix and add this back to the supp (maybe)
(2) If we keep the example, see Work on the Supp below for my ideas on this. 


Work on the Supp:
- Overview of data and question
- Show it does not work with lmer and backtrack to intercepts-only
- Step 1: 
	a) Simulate data with lots of data
	b) Simulate data with less data
- Step 2: Prior check
- Step 3: Run the model
- Step 4: PPC 

Co-author feedback (started 1 Jan 2024)

NEXT UP!
- Think a little on the new potential step 1 (step 1-2 merge) and ...
- Think a little on Will's request of what gap each step addresses
- Get some pen and paper or separate tex file notes on these two things above before diving in. 
- AND (have done the above) ... now go through paper text, making overview cuts/changes and finalizing paper notes

B) Finish Mike's (all his smaller comments in, so just these below):
- Perhaps ironically one of the important lessons of the paper — carefully analyzing assumptions and their consequences — kind of requires a detailed discussion to explain.
- Deal with his overall problem # 1 (below):

...but there is one systematic problem that I think should be addressed.

The main limitation with Step 1 as it is currently written is that it doesn't specify which "truths" to consider.  One can read it as looking at simulations from a single set truth or multiple set truths depending on one's expectations/experience.  

At the very least I think there should be a few sentences discussions the c choice of from **which model configurations to simulate ** (EMW emphasis).  Ideally for me Steps 1 and 2 would be exchanged so that the scope of the prior model is available to motivate which model configurations to consider, either informally by heuristically choosing a range of configurations across the range of the prior model or formally with prior predictive simulations.

This is much less critical, but there's an interesting discussion to be had about the problem of higher-order interactions.  Back in the day the identifiability of higher-order interactions in fixed effect and random effects models was a systematically studied problem (see for example Box, Hunter, and Hunter "Statistics for Experimenters") that involved crafting careful experimental designs so that the data could pin down these behaviors.  Much of that is no longer taught and so people end up applying the tools in situations where they silently fail.  The advantage of the recommended workflow is that these problems can be identified empirically so that even without all of that old-school experimental design knowledge one can use the tools more robustly.  That said this does tangent a bit from the main discussion, requiring a historical context that many are not likely to have, but it was striking enough to me that I thought it was worth mentioning even if it may not be appropriate for the manuscript.

* Lizzie thinks ... why not merge Step 1 and 2: Will added: % WDP: I am biased (as you know), but on reading this section I wonder if it could be merged with Step 1. You make an argument that makes me happy (the 100 parameters and 5 data points bit), and make a coherent argument (that won me over in person, and does in print) that using priors to simulate and check data is a good idea before fitting the model, as it checks the ecology of the system. So why not merge them, and have the reason for this being to check the ecology of the system? You could even use it as an example of why thinking of a model as statistics ("I must check my priors are uninformative") is the *old* way of thinking, whereas in the new way of thinking we ask "what can I learn about my model and system" (and so simulating from the priors is a way to check model behaviour and identifiability)?

\emph{Step 1: Check your model code.} 
\emph{Step 2: Check your priors.} 
NEW step 1: Check your model

Maybe. ... We need to get what is at once a nitty-gritty and fascinatingly insightful task out of the way first ... check your code. 

C) Will edits are added to tex, but need work, see diff: https://github.com/lizzieinvancouver/bayesianflows/commit/504385decb6f22dd1d1ff27a7c7002421ab7db5a

DONE:
- Added in comments from JD (see PDF in comments folder)
- Added in Mike's smaller comments

My musings:
- We may find we know way less than we think we did -- which is the pass to learn a lot more. 

<><><><><><><><><><><><>>
<> Refs on calibration <>
<><><><><><><><><><><><>>

- Simple and helpful: https://mc-stan.org/docs/2_25/stan-users-guide/simulation-based-calibration.html
- Handy: https://statmodeling.stat.columbia.edu/2019/09/05/gneiting-on-calibration-and-sharpness/
- Mike's stuff, which I have mostly covered but could review
- https://statmodeling.stat.columbia.edu/2020/10/15/calibration-problem-in-our-election-forecast/
- http://www.stat.columbia.edu/~gelman/research/unpublished/sbc.pdf (Validating Bayesian Inference Algorithms with Simulation-Based Calibration)

<><><><><><><><><><>
<> Misc stuff I did not fit in
<> from Mike's Sep-October comments
<><><><><><><><><><>

- How people often approach generating data  ... (see below email on 11 Sept 2023, especially lack of confounding part)
- In particular it’s often easier to tell a new story (model) than determine if an existing story (classical estimator) is good enough. ... Implicit to that is trying to figure out exactly what the existing story is encoded in a classical estimator.  


<><><><><><><><><><>
From a good Patreon monthly questions email:

More realistically we have to confront the fact that our practical observational models will not contain the true data generation process; at best we can track how well our inferences converge to good approximations to the relevant features of the true data generating process.

...

Another way of stating these critiques is that because of the default emphasis on irrelevant features and estimator artifacts many predictive scores poorly quantify actual risk, making them vulnerable to overfitting. All of these complications are why I basically avoid predictive metrics entirely, although I admit that isn’t a popular approach. Personally I’m not so much interested in selecting between multiple models but rather identifying the inadequacies of a single model and then using that to inform principled improvements. This is more readily achieved with visual retrodictive checks than awkward quantifications.


<><><><><><><><><><>
<> Mike's feedback
<> 30 Aug 2023 <>
<><><><><><><><><><>

STUFF I could integrate, but have not yet... 

Ecologists have to learn how to translate that domain expertise into probabilistic models; this isn’t trivial but it’s not as overwhelming as is often described.  In particular it’s often easier to tell a new story (model) than determine if an existing story (classical estimator) is good enough.

Step 1 as modeling the data generation process.

I wouldn’t say that you shouldn’t fit models that you can’t simulate but rather implementing a simulation first often makes the modeling process much easier.

Simulation-based study of potential inferential behaviors generalizes the calibration of frequentist estimators.

"Note that I use calibrate here in the sense of determining what outcomes a model might return" (Mike's workflow)																																																																																																																											


DONE: 
One of the common features of contemporary science is that data sets are getting larger, but also more messier. 

...for inferences to be meaningful models need to model the data generating
process, including the underlying ecology, the measurement design, and any contamination or corruption of the data. Classical methods are based on simple model assumptions, which are appropriate only when that data generating process is very clean.  One reason why the classical methods are becoming more fragile is that contemporary data generation processes are becoming dirtier.  The only way to do better is to spend way more time on cleaner experiments that are applicable to the classical methods or develop new methods bespoke to these more complicated data generating processes.  Of course Bayesian methods are particularly well-suited to the latter.


<><><><><><><><><><>
- Reply to Mike (sent 8 Sept 2023)

Hi Mike,

I am heading out for my annual September in the Rockies (Canadian Rockies) backpack so sending thoughts now, since I won't get any further for probably two weeks. I wanted to get further, but here I am. 

All your caveats sound fine.

And I loved your comments! I pulled out some that I have worked to integrate. I added some of the points of the data generating process to the intro and worked in inference versus calibration (yes!). Here's major things I wanted to add but did not yet work in (in no order):

(a) In particular it’s often easier to tell a new story (model) than determine if an existing story (classical estimator) is good enough.
(b) Best model isn’t based on fewer assumptions but rather _transparent_ assumptions.
(c) By adopting a careful workflow one doesn’t lose any of the benefits of existing methods; rather ....

And a few comments: 

(1) I think introducing the data-generating process is great, and I have worked to add it in I think we can add it other places too. That said, I don't want it to feel the focus because I think ecologists are often taught you only need Bayesian for messy data and when you have a specific data generating process. But ...

I have gotten way more out of the workflow -- I see through the stats (how bad p-values are becomes really clear when you simulate data; inference vs. calibration) and the science more deeply. And I want to somehow convey that clearly. This feels especially true just trying to get a new very good postdoc (fairly quantitative and very bright) postdoc in my lab to simulate data from y=ax+b linear regression. He did what *everyone* I know does -- he wrote lines and lines of code to mush together distributions until he had built some y data. He never built x data, he never assigned parameters and then he told me that his simulating y from y=ax+b was circular and that we should not use a normal distribution model because he doesn't have negative numbers in his y data .... The fundamental understanding of how to calculate an interaction, recode a categorical variable from hi/low to 0/1 etc.. has never been taught -- people have no idea the *values* their intercepts or slopes take, they don't see how simple equations add up ... and this workflow teaches that, or at least it did for me and has for others.  

Thus I am a little worried about (1st) just re-affirming to ecologists too much that you only need Bayesian for messy data with complex data generating processes and (2nd) missing the message that you can get a lot more from this workflow. Even for the simplest model doing this `workflow' can teach SO MUCH, IMHO. 

(2) Workflow of what? Good question! I feel a little embarrassed that I can't put my finger on this beyond, 'have a model you feel confident is functional for your aim (meaning to understand enough of how it works, what limits it has etc.).' 

(3) I'd like to keep the language as simple as possible. I'd also like to avoid new terms, but I think we *should* add a table or `glossary' that spells out how we're using some words. 

So, if none of this already makes you want to jump off the paper and you have any time in the next couple of weeks (or later), please send more thoughts (feel free to edit the tex etc.) and we can hopefully iterate on a draft we both like. BTW, I did not write the `A bluffer's guide to what Bayesian statistics is' so edit away there (though I added some of your improvements there ... not yet sure if they fit). 

Because I am typing this pretty fast on a Friday, I hope my email conveys how much I liked your comments and hope you'll keep iterating with me (but also, zero pressure!). 

L


<><><><><><><><><><>
<> August 2023 from Will ...
<><><><><><><><><><>

Great quote from Will's rant: Everything matters: it is the role of statistics to shine a light on what matters most.