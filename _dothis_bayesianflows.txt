Started 22 August 2023 

Email co-authors on 13 May:

Thanks for all your feedback on the last draft! I have integrated most of it (some was conflicting, so I made executive decisions) and have a new draft that I think is close to ready for submission. I'd like to try submitting to Ecology Letters as a `method' article to start for a couple reasons. First, it takes less formatting than MEE, and second, it's a new section (I think?) so they may be more excited to take our piece (hope springs eternal). If you suggest we not do this, let me know.

What I need your help with:
(1) Edits to the cover letter (bayesflows_ELEmethod). We need approval to submit a 'Method' so the cover letter is currently one asking for an invitation.
(2) Your affiliation info so I can start formatting for a journal.
(3) Some indication you are okay with this paper for submission. If you want to review the text again, I'd appreciate if you try to focus on only sending: (a) ideas to cut text and (b) typos or major errors. (You can also skip reviewing it and I will get it double-checked for typos.)

* Please reply by 17 May with edits to the letter so I can submit it. Ideally, send everything else by 24 May.*

Thank you!
Lizzie

PS: Just found this, so perhaps I will hack the letter down -- or not ... https://onlinelibrary.wiley.com/doi/full/10.1111/ele.14304



NEXT UP!
- Fix the cover letter -- see 'How to publish a ‘Method’ article in Ecology Letters' https://onlinelibrary.wiley.com/doi/full/10.1111/ele.14304 -- mention the case study, the high quality code etc. And probably SHORTEN some. 
- ADD affiliations!
- Decide if I want to tweak colors of simulated data versus real versus PPC etc. to match workflow any more?
- Submit to ELE 
- Submit to MEE as a review which seems to cover all versions of crazy (https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.14270 and https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.14108) or submit to ELE (https://onlinelibrary.wiley.com/page/journal/14610248/homepage/forauthors.html#tips2) as a method. Though we don’t really have a case-study … maybe we should just be a perspective? 

<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>>
<> Notes from working on co-author feedback (started 1 Jan 2024) <>
<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>>

Keeping some good notes from Mike here (they are also in a PDF somewhere). 

- Perhaps ironically one of the important lessons of the paper — carefully analyzing assumptions and their consequences kind of requires a detailed discussion to explain.

This is much less critical, but there's an interesting discussion to be had about the problem of higher-order interactions.  Back in the day the identifiability of higher-order interactions in fixed effect and random effects models was a systematically studied problem (see for example Box, Hunter, and Hunter "Statistics for Experimenters") that involved crafting careful experimental designs so that the data could pin down these behaviors.  Much of that is no longer taught and so people end up applying the tools in situations where they silently fail.  The advantage of the recommended workflow is that these problems can be identified empirically so that even without all of that old-school experimental design knowledge one can use the tools more robustly.  That said this does tangent a bit from the main discussion, requiring a historical context that many are not likely to have, but it was striking enough to me that I thought it was worth mentioning even if it may not be appropriate for the manuscript.

DONE:
- Added in comments from JD (see PDF in comments folder)
- Added in Mike's smaller comments.
- Added in Will's comments.
- Dealt with Mike's bigger comments. 

My musings (sadly, none of these fit):
- We may find we know way less than we think we did -- which is the path to learn a lot more. 
- Bayesian is often presented as an alternative approach to do the same thing you do in Fisherman with slight tweaks; in our experience it's a different way to do science.  
- Maybe. ... We need to get what is at once a nitty-gritty and fascinatingly insightful task out of the way first ... check your code. 

<><><><><><><><><><><><>>
<> Refs on calibration <>
<><><><><><><><><><><><>>

- Simple and helpful: https://mc-stan.org/docs/2_25/stan-users-guide/simulation-based-calibration.html
- Handy: https://statmodeling.stat.columbia.edu/2019/09/05/gneiting-on-calibration-and-sharpness/
- Mike's stuff, which I have mostly covered but could review
- https://statmodeling.stat.columbia.edu/2020/10/15/calibration-problem-in-our-election-forecast/
- http://www.stat.columbia.edu/~gelman/research/unpublished/sbc.pdf (Validating Bayesian Inference Algorithms with Simulation-Based Calibration)

<><><><><><><><><><>
<> Misc stuff I did not fit in
<> from Mike's Sep-October comments
<><><><><><><><><><>

- How people often approach generating data  ... (see below email on 11 Sept 2023, especially lack of confounding part)
- In particular it’s often easier to tell a new story (model) than determine if an existing story (classical estimator) is good enough. ... Implicit to that is trying to figure out exactly what the existing story is encoded in a classical estimator.  


<><><><><><><><><><>
From a good Patreon monthly questions email:

More realistically we have to confront the fact that our practical observational models will not contain the true data generation process; at best we can track how well our inferences converge to good approximations to the relevant features of the true data generating process.

...

Another way of stating these critiques is that because of the default emphasis on irrelevant features and estimator artifacts many predictive scores poorly quantify actual risk, making them vulnerable to overfitting. All of these complications are why I basically avoid predictive metrics entirely, although I admit that isn’t a popular approach. Personally I’m not so much interested in selecting between multiple models but rather identifying the inadequacies of a single model and then using that to inform principled improvements. This is more readily achieved with visual retrodictive checks than awkward quantifications.


<><><><><><><><><><>
<> Mike's feedback
<> 30 Aug 2023 <>
<><><><><><><><><><>

STUFF I could integrate, but have not yet... 

Ecologists have to learn how to translate that domain expertise into probabilistic models; this isn’t trivial but it’s not as overwhelming as is often described.  In particular it’s often easier to tell a new story (model) than determine if an existing story (classical estimator) is good enough.

Step 1 as modeling the data generation process.

I wouldn’t say that you shouldn’t fit models that you can’t simulate but rather implementing a simulation first often makes the modeling process much easier.

Simulation-based study of potential inferential behaviors generalizes the calibration of frequentist estimators.

"Note that I use calibrate here in the sense of determining what outcomes a model might return" (Mike's workflow)																																																																																																																											


DONE: 
One of the common features of contemporary science is that data sets are getting larger, but also more messier. 

...for inferences to be meaningful models need to model the data generating
process, including the underlying ecology, the measurement design, and any contamination or corruption of the data. Classical methods are based on simple model assumptions, which are appropriate only when that data generating process is very clean.  One reason why the classical methods are becoming more fragile is that contemporary data generation processes are becoming dirtier.  The only way to do better is to spend way more time on cleaner experiments that are applicable to the classical methods or develop new methods bespoke to these more complicated data generating processes.  Of course Bayesian methods are particularly well-suited to the latter.


<><><><><><><><><><>
- Reply to Mike (sent 8 Sept 2023)

Hi Mike,

I am heading out for my annual September in the Rockies (Canadian Rockies) backpack so sending thoughts now, since I won't get any further for probably two weeks. I wanted to get further, but here I am. 

All your caveats sound fine.

And I loved your comments! I pulled out some that I have worked to integrate. I added some of the points of the data generating process to the intro and worked in inference versus calibration (yes!). Here's major things I wanted to add but did not yet work in (in no order):

(a) In particular it’s often easier to tell a new story (model) than determine if an existing story (classical estimator) is good enough.
(b) Best model isn’t based on fewer assumptions but rather _transparent_ assumptions.
(c) By adopting a careful workflow one doesn’t lose any of the benefits of existing methods; rather ....

And a few comments: 

(1) I think introducing the data-generating process is great, and I have worked to add it in I think we can add it other places too. That said, I don't want it to feel the focus because I think ecologists are often taught you only need Bayesian for messy data and when you have a specific data generating process. But ...

I have gotten way more out of the workflow -- I see through the stats (how bad p-values are becomes really clear when you simulate data; inference vs. calibration) and the science more deeply. And I want to somehow convey that clearly. This feels especially true just trying to get a new very good postdoc (fairly quantitative and very bright) postdoc in my lab to simulate data from y=ax+b linear regression. He did what *everyone* I know does -- he wrote lines and lines of code to mush together distributions until he had built some y data. He never built x data, he never assigned parameters and then he told me that his simulating y from y=ax+b was circular and that we should not use a normal distribution model because he doesn't have negative numbers in his y data .... The fundamental understanding of how to calculate an interaction, recode a categorical variable from hi/low to 0/1 etc.. has never been taught -- people have no idea the *values* their intercepts or slopes take, they don't see how simple equations add up ... and this workflow teaches that, or at least it did for me and has for others.  

Thus I am a little worried about (1st) just re-affirming to ecologists too much that you only need Bayesian for messy data with complex data generating processes and (2nd) missing the message that you can get a lot more from this workflow. Even for the simplest model doing this `workflow' can teach SO MUCH, IMHO. 

(2) Workflow of what? Good question! I feel a little embarrassed that I can't put my finger on this beyond, 'have a model you feel confident is functional for your aim (meaning to understand enough of how it works, what limits it has etc.).' 

(3) I'd like to keep the language as simple as possible. I'd also like to avoid new terms, but I think we *should* add a table or `glossary' that spells out how we're using some words. 

So, if none of this already makes you want to jump off the paper and you have any time in the next couple of weeks (or later), please send more thoughts (feel free to edit the tex etc.) and we can hopefully iterate on a draft we both like. BTW, I did not write the `A bluffer's guide to what Bayesian statistics is' so edit away there (though I added some of your improvements there ... not yet sure if they fit). 

Because I am typing this pretty fast on a Friday, I hope my email conveys how much I liked your comments and hope you'll keep iterating with me (but also, zero pressure!). 

L


<><><><><><><><><><>
<> August 2023 from Will ...
<><><><><><><><><><>

Great quote from Will's rant: Everything matters: it is the role of statistics to shine a light on what matters most.


<><><><><><><><><><><><><><><><><><><><><><><>
<> How I handled March-April 2024 feedback <>
<><><><><><><><><><><><><><><><><><><><><><><>

Made the edits from Mike and Will. Trickiest was Mike mentioning we leap around from we/us/our to you/your ... I decided best was we to start, then you in workflow steps (mostly, one spot was us talking about us, I left that), then a mix (I tried switching to we after steps, but it did not work so I stuck with a mix). This was a GOOD change! I found a lot of weird 'we' in the steps, which was text I had pasted in too directly from Mike. 

Edits (deletions) from J on 13 March, not yet DELETED that I may want to re-consider:

- DELETE: using a number of skills often reserved in ecology more for ‘theorists’ than empiricalecologists. We argue this theoretical-vs-empirical divide is antiquated and limits progressby isolating the insights stimulated from model building. Further, it ignores that the averagemodern ecologist is computational, and thus already has many of the basic skills to build bespokemodels.

DELETED (noted in ms as: % DELETED 13MAY2024 for length): 
- After outlining the workflow, we discuss how it can benefit Bayesian inference generally, has improved our own science, and suggests a new way to train in ecology.
- (very end of paper): This workflow depends strongly on simulating data---for testing your model (Step 2), and understanding your model results (Step 4)---an area we actively under-train in ecology. It makes clear how relevant and important simulation is, but the relevance of simulation extends well beyond Bayesian model fitting. Simulation approaches allow interactive learning, build intuition, and stress exploring a model in its relevant---ecological---context. Ecologists are much better at thinking about ecological problems than statistical ones, and grounding our approach in ecology will likely bring the best out of our statistical modelling.

