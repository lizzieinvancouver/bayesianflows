\documentclass[11pt]{article}
\usepackage[top=1.00in, bottom=1.0in, left=1in, right=1in]{geometry}
\renewcommand{\baselinestretch}{1.1}
\usepackage{Sweave}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{gensymb}
\usepackage{parskip}
\usepackage{xcolor}
\usepackage{xr-hyper}
\externaldocument{bayesianflows}

\begin{document}

\section{What's a model?}


\section{What's a model?}

We cavalierly use the terms model, mechanistic model, process model, data generating process, and statistical model in this paper, which follows their current use in ecology.  This reality comes naturally from divergent fields using them in different ways, but it's important to recognize that what is `mechanistic' or `statistical' is not usually a clear distinction in ecology. Further, applying specific terms to certain modelling approaches, or to specific parts of a model should be done with care, especially if it impacts how you interpret the model. For example, consider: % The main problem with a lot of these terms is that they’re often used to imply deterministic processes, typically in a regression context where

$y \sim \text{normal}( f(x), \sigma)$.

Within this $f(x)$ is sometimes called the `mechanistic' or `process' model, and $\text{normal}(, \sigma)$, the `noise', `error', or even `measurement error', however, these terms are only accurate in certain (in our experience: rare) cases. Given that $f(x)$ will never contain the true underlying process, $\text{normal}(,\sigma)$ is functionally capturing everything not in $f(x).

\iffalse
On the more practical side of things you can’t simulate data, or calibrate estimators, without having model. Calculating power in a frequentist null hypothesis significance test requires the assumption of a data generating process, which is one reason why so many people avoid considering test power as much as possible.

A simulation of the form:

\begin{align*}
  \tilde{x}_n &\sim \pi(x) \\
  \mu_n & = \alpha + \beta * \tilde{x}_n\\
  \tilde{y}_n &= \pi(y; \mu_n)
\end{align*}


defines a certain data generating process where the covariates $x$ are generated first and then the variates $y$ are generated second, conditional on the covariates without any confounding whatsoever.  That data generating process might be relevant for modeling a particular ecological system, but it allows us to study the consequences of the regression assumptions.

This all ties back into the difference between calibration and inference.  `Calibration', or whatever we want to call it, has nothing to do with how well a particular data generating process models an actual system.  It’s just a way to investigate the assumptions encoded in a model and their consequences.  Only when we get to `inference' do we pair the model with real data and then address the adequacy of the model.
\fi



\end{document}

