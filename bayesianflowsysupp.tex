\documentclass[11pt]{article}
\usepackage[top=1.00in, bottom=1.0in, left=1in, right=1in]{geometry}
\renewcommand{\baselinestretch}{1.1}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{gensymb}
\usepackage{parskip}
\usepackage{xcolor}
\usepackage{xr-hyper}
\externaldocument{bayesianflows}
\usepackage{hyperref}

\def\labelitemi{--}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead[LO]{}
\fancyhead[RO]{}

\begin{document}
\bibliographystyle{/Users/Lizzie/Documents/EndnoteRelated/Bibtex/styles/besjournals}
\renewcommand{\refname}{\CHead{}}

\title{Supplement: Simulation as a best practice in Bayesian workflows and beyond}
\date{\today}
\author{EM Wolkovich, TJ Davies, WD Pearse \& M Betancourt}
\maketitle

\section{Which workflow?}

Formally, all a `workflow' does is organize various steps together in a systematic fashion, but there are many different workflows depending on what the aim is, which will determine which steps a workflow should include. For example a workflow aimed at calibration could look like an expanded version of our Step 1, where all the steps focus on investigating the assumptions encoded in a given model using simulated data. Or a workflow aimed at inference could expand Step 3, to focus on constructing a posterior, then investigating its model adequacy via several criteria. An inferential workflow can also be extended into a model development workflow.  If the model adequacy criteria inform not only that something is inadequate about the current model assumptions but what is inadequate (ideally this happens some in Step 4) then one can use those hints to iterative improve the modeling assumptions. We present in the main text a very simplified model development workflow that combines calibration, inference and some model development, but it is not necessarily appropriate for everyone, depending on their aims.

% Ultimately it may be helpful to advocate for workflows, plural.  Bayesian methodologies can be used to systematically investigate the consequences of a given model, such as an estimator calibration workflow.  They can also be used to formalize heuristic residual checking with posterior retrodictive check and implement an iterative model development workflow.  Etc, etc.  The goal is to identify what you want to do and organize the steps to achieve that output as systematically as possible.

\section{What's a model?}

We cavalierly use the terms model, mechanistic model, process model, data generating process, and statistical model in this paper, which follows their current use in ecology.  This reality comes naturally from divergent fields using them in different ways, but it's important to recognize that what is `mechanistic' or `statistical' is not usually a clear distinction in ecology. Further, applying specific terms to certain modelling approaches, or to specific parts of a model should be done with care, especially if it impacts how you interpret the model. For example, consider: % The main problem with a lot of these terms is that they’re often used to imply deterministic processes, typically in a regression context where

$y \sim \text{normal}( f(x), \sigma)$.

Within this $f(x)$ is sometimes called the `mechanistic' or `process' model, and $\text{normal}(, \sigma)$, the `noise', `error', or even `measurement error', however, these terms are only accurate in certain (in our experience: rare) cases. Given that $f(x)$ will never contain the true underlying process, $\text{normal}(, \sigma)$ is functionally capturing everything not in $f(x).

\iffalse
On the more practical side of things you can’t simulate data, or calibrate estimators, without having model. Calculating power in a frequentist null hypothesis significance test requires the assumption of a data generating process, which is one reason why so many people avoid considering test power as much as possible.

A simulation of the form:

\begin{align*}
  \tilde{x}_n &\sim \pi(x) \\
  \mu_n & = \alpha + \beta * \tilde{x}_n\\
  \tilde{y}_n &= \pi(y; \mu_n)
\end{align*}


defines a certain data generating process where the covariates $x$ are generated first and then the variates $y$ are generated second, conditional on the covariates without any confounding whatsoever.  That data generating process might be relevant for modeling a particular ecological system, but it allows us to study the consequences of the regression assumptions.

This all ties back into the difference between calibration and inference.  `Calibration', or whatever we want to call it, has nothing to do with how well a particular data generating process models an actual system.  It’s just a way to investigate the assumptions encoded in a model and their consequences.  Only when we get to `inference' do we pair the model with real data and then address the adequacy of the model.
\fi

\section{An example initial workflow}

We review our workflow using sample date related to the first Bayesian model one of us ever fit. The resulting model is not ideal (as we'll show), but we use it to highlight the reality of the learning the skills of this workflow. Much like learning a foreign language you improve over time, to where you almost cannot fathom where you started. And when you're first learning a foreign language the stories you can tell will be crude and  clunky, but as you become more proficient they'll become more elaborate.


Step 0.

You’ll be generating your model---including its priors---as you ask and try to answer questions about your proposed model. Effective model building is about efficient brainstorming.  It's a constant back and forth between asking
questions about what we know and what we should know.

Step 1... 

I mportant to emphasize that model configurations, i.e. parameters, have to be fixed first and then data simulated condition on that configuration.  For one the comparison between inferences and `truth' are a bit more clear": we're comparing what we estimated to what we started with.  For another it raises the important question of which model configurations we should try as relevant "truths", a question that eventually leads to prior predictive simulations.

Say this again here: I mentioned this last time -- I'm not sure if it's worth explicitly mentioning that this kind of simulation study is
exactly how power is formally defined.  Simulations allow us to estimate power in more general conditions that what is
typically assumed for analytic results.

Step 2 ...

Step 3....

Step 4 ...

I think that it would also be useful to mention residual analysis. Overall we need to compare model retrodictions to the observed data, projected onto the features relevant to the analysis. Classical residual analysis compares point predictions to the observe data. R2 summarizes this comparison as a linear correlation. Posterior retrodictive checks compare an entire distribution of predictions to the observed data, allowing us to not only identify any inconsistencies but also use the probabilistic spread of the predictions to qualify how important any inconsistencies are.

Feedbacks 

`Developing simulated data to test the model, running prior and retrodictive checks all dive you deep into understanding
your statistical model, which suddenly you may find yourself thinking through much more mechanistically.' I wonder if
it's beneficial to frame this more as `dive deep into connecting your statistical model to your domain expertise'?  I
often find myself trying to emphasize that people already have the domain expertise but they're not incorporating it into
their analyses.  A lot of what this workflow is trying to do is facilitate that integration to leverage _what the analysts
have always had!

\section{References}
\vspace{-5ex}
\bibliography{refs/bayesrefsmini.bib}
\end{document}
