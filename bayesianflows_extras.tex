
Bayesian approaches are not new to ecology. They have long been used in certain subdisciplines related to infer population sizes of things people want to eat or manage. These are also cases where the underlying biological model was well established, and underlying data collection was especially messy. For example, wildlife biologists frequently use mark-recapture data and associated models to estimate population sizes; such methods almost always use Hidden Markov models to link from the data generating process (where all individuals are not always perfectly observed) to the population model, which can rarely be fit without Bayesian methods \citep{muthuku2008,zheng2007,strinella2020potential}. Similarly fisheries biology has tended towards more complex models---such as state-space models---to estimate stocks, which address similar issues and are similarly easiest to fit with Bayesian approaches \citep{trijoulet2018,millar2000}. % For decades, Bayesian training in ecology has focused on these aims, but as data and methods change, so has how ecologists are using Bayesian models.

Away from these fields, however, ecologists have largely focused on frequentist statistics and null hypothesis testing. Such approaches seem superficially straightforward, and centuries of effort have led to standardized ways to address many particular questions. These approaches, however, are based on rigid assumptions that often don't hold for many ecological datasets.  For example spatial, temporal and phylogenetic correlations often violate the independence assumptions inherent to these methods. % This approach can, however, mean underlying assumptions are ignored, as spatial, time-series and phylogenetic data introduce non-independencies. 
Ecologists try to address some of these challenges for larger, more complex datasets by fitting multi-way interaction terms, using random effects to correct for group level factors, or comparing a cross of suite of models. But the outcomes are not often aligned with our ultimate aims:  multi-way interaction terms can make main effects hard to interpret, null hypotheses tend to be rejected even when the underlying effect sizes are weak \citep{gelmanhill,muff2022rewriting}, and many model comparison approaches prefer models whose inferences match the idiosyncratic fluctuations in the ecological data, but donâ€™t generalize to other observations. These practical realities highlight that our current approaches are not meeting the challenges of our data combined with our ultimate aims in ecology today.



The average ecologist today has a diversified skillset compared to an ecologist of decades ago---one that could potentially meet this challenge. Ecology has long been a field with deep statistical training, but the modern ecologist is now also expected to be computational: to handle large datasets, produce repeatable workflows, and translate models into forecasts for policy and planning. Many ecologists now bridge field, lab and computational methods. Bayesian methods can help facilitate this new job description, while also helping meet the demands for better models and forecasts. % As such, the rise of Bayesian approaches is especially timely for ecology. % The average modern ecologist is computational, but often grounded in the natural history of a particular system (err, maybe, on both these accounts, but whatever).

Today, as large-scale ecological data becomes available for more diverse systems and for questions addressing other aims, ecologists are using Bayesian models in new ways. Unfortunately, there is naturally a lag in training in statistics, and in Bayesian modeling in particular. Bayesian approaches provide a pathway to powerful models that can transform how we understand our systems, but they can also lead to pitfalls most ecologists are not trained to notice or manage. Many of these pitfalls can be avoided by approaching Bayesian analyses through specific workflows \citep{betanworkflow,vandeschoot2021}, which themselves are built on a process of how to do not just statistics, but how to do science. % Maybe address workflow comment here? 

These workflows---and the one we describe below---are focused on simulating data from models at multiple steps. This is a ground-shift from most statistical training in ecology, which has focused strongly on null hypothesis testing, but a timely one. As the modern ecologist of today is often computational, simulation methods are a more natural extension---and one that can provide insights and build intuition into both statistics and the natural systems we study. % Including simulation more consistently into how we build statistical models allows more interactive learning, easier tests of power, and builds intuition into both statistics and the natural systems we study. It can also help meet the demands on ecology for better models and forecasts. 
