\documentclass[11pt]{article}
\usepackage[top=1.00in, bottom=1.0in, left=1in, right=1in]{geometry}
\renewcommand{\baselinestretch}{1.1}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{gensymb}
\usepackage{xcolor}
\usepackage{xr-hyper}
\externaldocument{bayesianflowsysupp}
\usepackage{hyperref}
\usepackage{lineno}

\def\labelitemi{--}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead[LO]{}
\fancyhead[RO]{}

\begin{document}
\bibliographystyle{/Users/Lizzie/Documents/EndnoteRelated/Bibtex/styles/amnat}
\renewcommand{\refname}{\CHead{}}

%%%%%%%
%% To do %%
%%%%%%%
% https://github.com/lizzieinvancouver/gelmanhill/wiki/Vocabulary

\title{A four-step simulation-based workflow for ecology}
% Simulation as a best practice in Bayesian workflows and beyond
% How to fit Bayesian models and influence people
% How to do Bayesian model fitting in ecology 
% The best way to be a Bayesian in ecology today
% \date{\today}
\author{EM Wolkovich$^{1*}$, T Jonathan Davies$^{1,2}$, William D Pearse$^{3,4}$ \& Michael Betancourt$^{5}$}
\maketitle

\noindent $^{1}$ Forest and Conservation Sciences, University of British Columbia, Vancouver, BC V6T 1Z4, Canada\\
$^{2}$ Botany, University of British Columbia, Vancouver, BC V6T 1Z4, Canada\\
$^{3}$ Department of Life Sciences, Imperial College London, Ascot SL5 7PY, United Kingdom \\
$^{4}$ Alan Turing Institute, British Library, 96 Euston Road, London NW1 2DB, United Kingdom \\
$^{5}$ Symplectomorphic, LLC, New York, NY 10026, USA \\
$^{*}$ \url{mailto: e.wolkovich@ubc.ca}
 
\vspace{3ex}
\noindent \emph{Keywords:} big data; scientific workflow; data simulation; prediction; null hypothesis testing, machine learning
\vspace{3ex}


\newpage
\linenumbers
% about 140 words (200 max for NEE); main text is 4,000; 3-4K for a perspective, see https://www.nature.com/natecolevol/content
% Or submit to ELE as method: https://onlinelibrary.wiley.com/page/journal/14610248/homepage/forauthors.html#tips2
% Mid Jan 2024  >7,000 words // 4 Feb 2024: Down to 5023 // 7 April 2024: 4859 // 27 May 2024: 3713 // 26 Sept 2024: 2685
\abstract{Ecology is a discipline facing increasing challenges. Growing demands for useful forecasts to help manage systems requires models that can capture the variability and underlying uncertainty of ecological systems and related data. Yet many ecologists are not trained in current methods to build flexible robust models. Thus, they often rely on a limited set of pre-defined models combined with null hypothesis testing or continually take up new approaches without understanding their assumptions and limitations. The result is poor models that lead to incorrect predictions and decisions, alongside concerns of a looming replication crisis---a result not unique to ecology. 
% Yet many ecologists today are not trained to take advantage of the bigger ecological data needed to generate more flexible robust models. 
Here we describe a broadly generalizable workflow for statistical analyses and show how it can lead to better models and enhance training in ecology, with potential extensions to other fields. 
% Here we describe a broadly generalizable workflow for statistical analyses centered on simulating data from models, and show how it can enhance training in ecology. 
Building on new advances in workflows from statistics and the increasingly computational toolkit of many ecologists, this approach leverages simulation to integrate model building and testing for empirical data more fully with ecological theory. It leverages new advances in workflows from statistics, that have yet to be widely used in ecology. We argue this approach can fit models that are more robust and better-suited to providing new ecological insights and improved predictions, and may provide a blueprint for other fields similarly challenged by complex systems, growing datasets and limited training in how to best approach them. }
% OLD version of second half (including our Bayesian beach): Here we present a Bayesian workflow---centered on simulating data from models---that represents a ground shift of not just how to fit Bayesian models, but how we should approach model fitting to advance our science. This workflow integrates mechanistic and statistical models with a computational toolkit, which we argue can accelerate ecological science. By interrogating our methods through generative models, we can refine where to put resources for better estimates, better models, and better forecasts. While we outline these methods from the happy surf of our Bayesian beach, the ocean we describe works for anyone fitting models to ecological data.

\vspace{3ex}
% \noindent \emph{Data availability:} No new data, but full code and raw data are provided in the supplement for an example workflow.
\noindent\emph{Example \& Data:} We provide an example of the workflow with complete code available at \url{https://github.com/lizzieinvancouver/bayesianflowsexample}.  The data used for the example is provided and full metadata on it is available via the Knowledge Network for Biocomplexity: \verb|doi:10.5063/F12J69B2|. 



\setlength{\parindent}{0pt}
\setlength{\parskip}{7pt}

\newpage

\section*{Introduction}
In recent decades, the field of ecology has become more policy-relevant as growing anthropogenic pressures have increased the need to manage and understand ecologists systems  \citep{hak2016sustainable,lindenmayer2010science}. As ecologists have worked to develop predictive models to meet these demands they have developed ever larger datasets \citep{Hampton2013}. These bigger data, however, are also messier data. Such data generally require more complex models, such as those that accommodate both the underlying biological processes and how the measurements were made. Some fields have long used these types of models \citep[generally in fields focused on inferring population sizes for management,][]{muthuku2008,zheng2007,trijoulet2018,strinella2020potential}. Most, however, have not. This has left a number of ecologists to try to adapt what they were trained in---traditional statistical methods (e.g. $F$ and $t$ tests) and a strong focus on null hypothesis testing---to increasingly complex datasets. 
% Often they have done this by fitting multi-way interaction terms, using random effects to correct for group level factors, or comparing across a large suite of models. 
% Often they have done this by fitting multi-way interaction terms (rather than using mechanistic models informed by biological understanding), using random effects to correct for group level factors (rather than explicit models of data-collection biases), or comparing across a large suite of models (because they cannot fit a single model that accounts for all the idiosyncrasies and biases) ... and so aim to average out the noise to leave only the signal 

Yet many common statistical approaches do not align with the demands on ecology today. Beyond the reality that most traditional methods are fragile when used beyond the cleaner, simpler experiments these methods assume (e.g. spatial, temporal and phylogenetic correlations often violate independence assumptions), they will usually fail to produce robust, reproducible results. For example, an overly zealous focus on $p$-values has led to a replication crises in several fields, where results seem most likely the outcome of noisy data combined with a search for statistical significance through many models \citep[effectively a garden of forking paths,][]{halsey2015,loken2017}. Some model selection approaches, including new machine learning methods, try to avoid this by comparing across models, but may not generalize to provide useful forecasts \citep{boettiger2022forecast}. This is especially true when forecasts have to adapt to changes in the underlying biology. These leaves ecology in a predicament shared across other fields---concerns of a looming replication crisis \citep{filazzola2021replication,fraser2020role} and overly confident forecasts with the potential to erode public trust in science \citep{leroux2019prevalence,boettiger2022forecast}. 

Many ecologists recognize these issues and have turned to methods better designed for forecasting from complex systems and messy data. Bayesian methods \citep{anderson2021trends}, which can handle many of the complexities of ecological data \citep{hobbs2006alternatives}, have increased in use. Because they allow a huge diversity of bespoke models and increasingly can accommodate large datasets Bayesian methods provide a potential pathway to transform how we understand our systems as more ecological data become increasingly available. New algorithms \citep[e.g. Hamiltonian Monte Carlo,][]{nuts2014,betan2019} that have made fitting and implementing Bayesian models faster, more robust and---in many ways---easier \citep{Carpenter:2017stan}. At the same time, machine learning methods are also increasing in use \citep{pichler2023machine}. These methods, which benefit from large datasets---often with many predictors---to make predictive models via test and training datasets \citep{breiman2001statistical}, have revolutionized image detection in ecology, but are increasingly used to forecast ecological processes \citep[e.g.,][]{zwart2023near}. Machine learning methods often build complex, opaque models \citep{CoxTwoCultures,EfronTwoCultures,shmueli2010explain}, thus providing opaque inference into ecological processes, but efforts are underway aim to change this \citep[e.g.,][]{kutz2023machine}. 

Regardless of the approach, fitting larger and often more complex models presents challenges that are frequently not addressed in traditional ecological training. We suggest that many of these challenges can be overcome by approaching analyses through specific workflows \citep{betanworkflow,grinsztajn2021,vandeschoot2021}, which themselves are built on a process of how to do not just statistics, but how to do science \citep{box1976science}. While these approaches are slowly gaining traction in other fields \citep[e.g.,][]{esfahani2021bayesian,schad2021,bouman2024bayesian}, they are not widely used in ecology today. Such approaches move away from a focus on null hypothesis testing, towards estimating effect sizes, using models calibrated (see Table \ref{tab:glossary}) and better understood through simulating data at multiple steps. % These workflows use a number of skills often reserved in ecology more for `theorists' than empirical ecologists. But this theoretical-vs-empirical divide ignores that the average modern ecologist is computational, and thus already has many of the basic skills to build bespoke models \citep{hilborn2013ecological}. 

Here we outline a simplified---but powerful---workflow that builds on new insights from statistics  \citep{betanworkflow,gelman2020bayesian,vandeschoot2021} and the increasingly computational nature of ecology today. Our aim is to provide an approachable rubric for those new to fitting complex models or simply those interested in re-considering their current workflow (and is not intended to be a comprehensive overview; see `Next steps' in the Supplement). Because of this aim, we provide examples of simple models, and suggest resources as users build more complex models. Our examples include several statistical inference methods, though we focus more on implementing the workflow through a Bayesian statistical framework (with an example in the supplement shown in \textsf{R} and \textsf{Stan}), because this framework allows integrating bespoke model building more fully with ecological theory and understanding.  We argue that this approach can help fit models that are more robust and well-suited to provide new ecological insights---allowing us to refine where to put resources for better estimates, better models, and better forecasts.
% While we believe this workflow may be most critical for fitting complex models, learning it is easiest starting with simple models and thus we focus our examples---across multiple inference methods---there. 
% Our aim is not to provide a comprehensive overview, but instead to provide an approachable rubric for those new to fitting complex models. 
% By integrating bespoke model building more fully with ecological theory and understanding---and vice versa---this approach can fit models that are more robust and better-suited to providing new ecological insights and improved predictions.

% After outlining the workflow, we discuss how it has improved our own science, and suggests a new way to train in ecology.
% Once you start doing this workflow, your scientific life will never be the same. 

\section*{A four-step workflow}
% \section*{A short guide to statistical workflows and Bayesian approaches}
% Statistical analyses are designed for inference---to learn about some process, effect or behavior from data. Robust analyses yield inferences consistent with the underlying truth more often than not (quantifying this consistency is called calibration, and is a critical part of using models for inference, see Table \ref{tab:glossary}). Because we do not know the `truth' how we approach our analyses---and our inference---is a critical component of how we do science. Fragile statistical analyses can lead science away from the type of repeatable and generalizable findings expected when inferences align with the truth. 

Our workflow outlines what we consider the major steps for building bespoke models (Fig. \ref{fig:workflow}). Many of these steps will be familiar to statistical ecologists, but are often overlooked, whereas other steps may appear particular to Bayesian methods (e.g. prior predictive checks), but are actually useful for anyone---using Bayesian models or not---to challenge their models of how the world works. We find a Bayesian framework the easiest way to apply this workflow (see \emph{A brief review of statistical inference using Bayesian approaches} in the Supplement), especially for building bespoke models, but we touch on other approaches throughout. We argue this workflow is useful for all approaches (Fig. \ref{fig:twoexamples}-{fig:retrodictivecheck}), and may be especially important to understand how new techniques to ecology, such as those from machine learning \citep{pichler2023machine}, actually apply to help answer ecological questions.
Parts of this workflow could be expanded as workflows in themselves, given other aims (see Supplement: Which workflow?). % For simplicity, many of the smaller but still critical steps are omitted, including visualization, which is required at every step \citep[and for which there are many good resources, e.g.][]{gabryvis}. 

\subsection*{Step 1: Develop your model(s)} 

We start the workflow with what can feel like the biggest step---build a model (or potentially, models) based on your aims \citep{hilborn2013ecological}. By developing a model designed for your biological question, data and aims, your statistical workflow naturally becomes a scientific workflow. You will more clearly see the assumptions and mechanisms in your model, which is especially valuable given how often our intuition of how models `work' is wrong \citep{kokko2005useful}. You likely already have a model, though it may be only verbal or conceptual. For this workflow, however, you’ll need to convert such models into mathematical versions \citep{servedio2014not}. % This step is better approached before you collect your data. After data collection, it becomes far more tempting to focus on the particular details in the data and not the latent processes and biological models from which the data were generated. 

Though it can feel challenging at first, this step is best approached before you collect any data. A suite of resources for `generative' or `narratively generative' modeling can help \citep{statrethink,betangen}. % along with two points. First, know that you can and will improve on this skill. 
As you start, ask lots of questions---and push yourself on your answers---about what you expect and what's reasonable biologically from your model. % For example, instead of simply identifying which distribution your observed response variable looks most similar to, ask yourself what ecological processes generate that distribution and how they influence its properties (e.g. its mean, minimum and maximum). Do you expect data below zero? Up to infinity? If not, why not? Effective model building is about efficient brainstorming and this is a critical part of the process. 
As you do this, you'll be generating your model---including its priors. Priors are important for Bayesian analysis, but the basic idea of them---coming up with a distribution of reasonable values for parameters in your model (see Table \ref{tab:glossary})---is useful to all analyses. Assigning priors generally forces you to think about your model with regards to your study system, and interrogate what's probable, possible or actually unreasonable---and can quickly disabuse users of prejudices regarding priors. For example, you may not think you have a prior on how sunlight affects plant growth, until you realize your `agnostic prior' actually allows plants to grow hundreds of meters per day. % You can take this a step up with Step 2's prior predictive checks. % something like `and generally this will not impact your results, see Supp'

\subsection*{Step 2: Check your model on simulated data} 

Once you have your model and its priors jotted down, you need to write up your model in a particular modeling language and check it. As with all code: just because it runs, does not mean it does what you think it does. The worst errors often still permit code to run. % Whether writing it out in \textsf{Stan}, where you need to be able to write out the full likelihood and set all your own priors, or using a package that writes much of the model for you (e.g. \textsf{rstanarm}), you need a way to verify the code is correct: test data.

Test data (aka `simulated data', or  `fake data,' etc.), and the skills required to build it, are central to this workflow. With `test data' you simulate data from your model in such a way that you can use the resulting data to test if your model code is correct (i.e., you fix values for your model parameters, then test how well your model recovers them, see the Supplement for an example). While there's no guarantee that inferences will always recover the parameter values you set, even when using the correct model, extreme disagreement is often an indicator that something is amiss in the implementation of the model. At the same time these simulation studies can help understand how often a model might lead to the correct inference (see Figs. \ref{fig:twoexamples} and \ref{fig:misspecifyprior}).  As you do this, you will also be calibrating your model---seeing how accurately and precisely it estimates parameters and under what conditions. 

This very basic model checking step is uncommon for many ecologists, but critical in our view. If you can simulate data from your model, then you can powerfully---and easily---answer questions related to statistical power, what effect sizes are reasonable, and---most likely---have new insights into how your model suggests the world works, all before looking at any real data. Thus, this apparently simple programmatic task actually encapsulates a far deeper understanding of your model. `All models are wrong; some models are useful,' becomes much clearer when you have the power to generate data from your model under any parameter set and sample size you want (see Fig. \ref{fig:twoexamples} and related Supplemental examples). 

You can learn only so much, however, from data simulated from a particular parameter set. Simulation studies across multiple parameter sets allow you to investigate how robust your inferential performance might be. Prior predictive checks \citep{betanprior,winter2023} use the Bayesian prior model to set this scope of such simulations, but can be used in any analysis. For these, you draw values from your prior distribution and then explore how your model performs. Seeing how this influences your resulting output reveals the extent to which your model can capture known variation in your data, and gives insight into whether your model is capable of distinguishing among competing hypotheses. % It also serves as a check on the priors you're using (addressing one of the common concerns of those inexperienced with Bayesian models). 


 \subsection*{Step 3: Run your model on your empirical data} 
 
The next step is to run the model---you've now evaluated, test-run and have ready to go---on your exciting new empirical data. Check diagnostics so you know it's running well and adjust until it is \citep[this includes a suite of convergence and efficiency metrics that are well-discussed elsewhere,][]{betanworkflow,gelman2020bayesian,vandeschoot2021,gabryvis}. % A model that doesn't converge, or seems to suggest coefficients that are completely at odds with the data, can result from a model that was mis-specified and could never capture real-world variation. For serious problems you may need to reevaluate your model entirely and return to Steps 1 and 2.
 
This is the step many ecologists skip straight to, ourselves included. It's easy to see the appeal: this is the inference step and where you might gain new ecological insights. Fitting new data to the model can feel like the moment when you'll learn something new. But, at least in our experience, this is not always the case. When we rush to this step, that first model we fit is often followed by another, and another---perhaps because one does not converge, or the results of another do not make immediate sense. And with the excitement of getting a model to run we can get distracted from what we are actually most interested in---the inference into ecology. %In contrast, by approaching the model through Steps 1-2, it's often much easier to quickly see through the results of the model fit. 

Following this workflow can make this step much more satisfying. Here the benefits of the workflow may become more apparent: you have estimates in useful units with uncertainty you can understand. You can use this information to draw new conclusions, design new experiments and more---but this is also a point to stop and check your model. 
%  As we're all familiar with this step, it's hopefully straightforward. Run the model---you've now validated, test-run and have ready to go---on your exciting new empirical data. Check diagnostics so you know it's running well (convergence metrics, lack of divergent transitions etc.) and tweak until it is. 

\subsection*{Step 4: Check your model on data simulated from your empirical model output (also known as posterior retrodictive checks)} 

% Once you have your posterior based on your model and new empirical data, it's time to interrogate it. 
Once you have your parameter estimates based on your model and new empirical data, it's time to remember that your model is wrong (as all models are) and ask how useful it is. You can do some of this through common model-fit diagnostics, such as $R^2$, which compares point predictions to the observed data. With a Bayesian posterior (see Table \ref{tab:glossary}), however, you have an added benefit in that you can compare an entire distribution of predictions to the observed data. 

This is where simulating from your model can be especially insightful. It will not only indicate that the model isn't adequately fitting the data but also can suggest what the problems might be. Using the parameter estimates from your posterior to simulate new data \citep[][]{held2010,gelman200ppc,conn2018} lets you see how that new world compares to the observed data---called posterior retrodictive checks or posterior predictive checks (Fig. \ref{fig:retrodictivecheck}). This is most easily done in a Bayesian framework where your posterior captures your uncertainty in a useful way, but can be done with estimates of your parameters and their uncertainty from other inferential frameworks. 
% Steps 1-2 have set you up well for this, as you have a sense of what different parameter estimates do to the model, and test data provide a sense of how it works on data similar to yours. 

Often here you may find big differences from your empirical data, and can start to generate hypotheses for why. For example, you may find patterns that suggest missing grouping factors (e.g. site or biome) through visual posterior retrodictive checks, or you may quickly realize your model predicts impossible numbers for your biological reality. You may begin to see inadequacies in your model, or even potentially your data.  This is one of the main benefits of the workflow: models don't fail silently, they fail with a wealth of context that helps to generate new models and experiments.
% it helps generate new models and experiments. 

\subsection*{Feedbacks \& workflows}
A key feature of this workflow is that it can be iterated.  If you find that you want to tweak your model then you return to the beginning, adjust your model, and repeat the rest of the workflow. In this way, fitting multiple models is encouraged, but this is distinct from the quest for a minimum adequate model or one `best' fit. Feedbacks in this workflow are focused far more on what is biologically reasonable, and understanding the utility---and limits---of inference from your data for your model.  And there are big benefits to it. 


\section*{How this workflow changed our science} 
Before this workflow, not all of us commonly discussed the values that parameters in our model took---things like the slope and intercept (two common model parameters) were sometimes reported, but we did not know them as well as we knew whether the $p$-value for the slope was $<0.05$. This changes quickly when you need to build simulated data (Step 2). For example, when modeling phenological events (observations of biological events on numbered days within the calendar year: 1-365 most years) it is not uncommon to find seemingly-reasonable models generating predictions of events on the non-existent calendar day of 1000. % This focus on the value of parameters scales up through this and other modeling workflows. Having a better sense of parameter values across different biological contexts, model parameterizations, and time periods gives a better sense of how the biological world works, including what's reasonable, possible or wildly unrealistic. % Bayesian methods naturally keep our focus on parameter values.

A closer inspection of our parameters also taught us a lot about identifiability and nonidentifiability, which refers to when all parameters in a model can---or cannot---be uniquely identified with infinite data, and a statistical kin: degeneracy (see Table \ref{tab:glossary}).  Degeneracy concerns the kinds of complex uncertainties that can arise from finite data sets \citep{gelmanhill}, and something we have often found in Steps 2-3 of our workflow.
% Statistically, a kin of nonidentifiability---degeneracy---is often an outcome of the empirical data combined with the model (thus appearing in this workflow often in Steps 2-3). Degeneracy occurs when the data do not contain enough information to estimate one or more more parameters uniquely \citep{gelmanhill}. 
Nonidentifiability and degeneracy can come up in many ways in ecology, and make us think we understand processes we do not. We never noticed them before using this workflow, but since then we have realized (especially in steps 1-2) lots of places for nonidentifiability and degeneracies to live---and we have adjusted how we collect data and interpret results because of it. For example, we have found fitting both site and species in a model with highly imbalanced data or trying to estimate interaction terms with low sample sizes \citep[][for more details]{regotherstories} leads to degenerate models, but there's often no warning in packages to tell us this. 

% For example, when species do not occur across most sites, a model including separate parameters for site and species is often degenerate, but there's often no warning in packages to tell you this. % We have become far better at noticing nonidentifiability and degenercies based on this workflow 

% Once we noticed how pervasive non-identifiability and degeneracies are how our approach to building complex models also changed.
%Whereas previously when we had data that qualitatively appeared complexly nested, crossed, split or twisted, we would have initially tried to fit all of these intricacies (on the intercept), we are now more slow to add these to our models. By both understanding these terms better (including the many different ways each can be modeled), and understanding them better depending on the data and model in each unique context, we now work more carefully through what to include. 
% Before using this workflow, we might start with more complex models, then simplify them until their fits converged in our given software package. Often these were hierarchical models with many levels---for example, including every column of site, plot, transect and quadrat in our dataframe, without stopping to check how well sampled they were, or what degeneracies they might introduce. % These models we built were based more on fear of missing an important interaction term or on a dogma of non-independence (and, somewhat relatedly, correct degrees of freedom) than an understanding of the model and the system's ecology. Our approach was driven far more by a weak set of statistical assumptions constrained by our software package, rather than by our ecological understanding or ultimate aims. 
% We also often fit a suite of interactions: multiple two-way interactions and the occasional three-way interaction were common fare. But in simulating data, and fitting models to real, messy, imbalanced data using the workflow we came to see how much we were asking of our data and models together. Fitting a two-way interaction with half the effect size of a main effect takes a 16X sample size, compared to fitting the main effects alone \citep[the main effects then average over the interactions, see][for more details]{regotherstories}. This is sobering. It's more sobering when you see it played out again and again through this workflow. % WDP (Apr2024): The above is grammatically perfect, but a bit tricky to parse, because you're using the present tense to describe what was done before. I see you're doing it to have greater impact and clarity in most of the paragraph, which I like, but then the final bit becomes unclear and implies we have issues in our workflow "see it played out again and again through *this* workflow". Can you modify a bit?

%\section*{How this workflow changed our science} % Surprising things that happened to us that may happen to you if you follow this workflow

\section*{How this workflow intersects with ecological training} %Challenges of the workflow compared to how we traditionally train ecologists

This four-step workflow is a simplified version of the current best practices for model fitting  \citep{betanworkflow,vandeschoot2021}, but many of the skills required are not part of traditional ecological training. Writing out the math behind most statistical models enough to complete Steps 1-2 bleeds into the skillset usually reserved for those working on theory, where coding and simulating from a model are common tasks. In contrast field, lab and otherwise empirical-data based ecologists often fit models they could not simulate data from. This dichotomy seems short-sighted in our current era of bigger, messier data and greater computational methods poised to handle such messy data. Further, the increasingly computational toolkit of the modern ecologist makes it easier to bridge the gap between ecological models and their underlying math. 
% Further, given the increasingly computational toolkit of the modern ecologist, training in how to simulate data from models may be a smaller leap than decades ago.  

%While simulation methods may appear foreign initially, they are usually much easier to implement than the analytical derivations of traditional methods so often seen in textbooks and classes.   % We believe training ecologists with a skillset where they quickly can use this workflow could advance ecological science rapidly. % Theory = simpler models = outcome of a good Bayesian workflow (often)
We argue training in simulating data as part of an organized workflow could speed progress in ecology and is possible given the current skillset of many ecologists. A reasonably competent coder could easily simulate data under a complex model that they might not have the mathematical expertise to solve analytically---if doing so was part of their training and the workflows they regularly use. While training in frequentist methods often includes memorizing assumptions for a particular test, or steps specifically designed to test particular assumptions (e.g. normal quantile plots), this workflow requires no such training. Instead it requires only the skills to identify whatever the assumptions have been encoded in your models. As such it moves away from some modeling paradigms in ecology, which focus on fewer underlying assumptions (e.g. random forests, non-parametric), to building models were the assumptions are transparent and motivated by the specific domain expertise available in each application. % Again, this is an area where those trained for theory often gain these skills, but it doesn't have to be that way, man. 

Advances in developing workflows have come alongside improved algorithms, visualizations \citep[e.g.][]{betanworkflow,vandeschoot2021,gabryvis}, perspectives on priors \citep{BDA,regotherstories,betanprior}  and hierarchical approaches that could also improve training. % Best practices for determining priors today \citep{BDA,regotherstories,betanprior} include that mathematically-convenient (e.g., conjugate) priors are no longer necessary, that `non-informative priors' are a misnomer, as they are often informative  \citep{lemoine2019}, and priors can easily be `weakly informative.' Thus a strong training focus on the dangers of priors is unnecessary, especially when ... 
For example, new work shows that prior predictive checks provide a more powerful and intuitive way to understand how priors work within a particular Bayesian model \citep{betanprior}, compared to past approaches. Similarly, traditional ecological training in hierarchical models still often refers to grouping factors (such as species or individual) as `random effects,' which is misleading, imprecise and thus no longer recommended \citep{gelmanhill}. In ecology, it also carries with it many older `rules' of what is `random' versus `fixed,' including that `random effects are things you don't care about' (for example the `block' effect from a randomized block design). Training in posterior retrodictive checks (Step 4) may reshape these views, as hierarchical effects are (by definition) drawn from an underlying distribution---meaning they can predict outside of the specific set  sampled (for example, to predict for a new species or individual), whereas the same is not true for most categorical `fixed' effects.

\section*{How this workflow extends to other fields} 

These new best practices have gained traction at the same time that ecology, alongside many other fields, has recognized that $p$-values, and null hypothesis testing in general, are easily misleading \citep{gelman2017statistical,ferraro2020feature,filazzola2021replication,fraser2020role}. Small sample sizes alongside a tendency to fit complicated models with multiple interactions makes ecological research vulnerable to these problems \citep{gelman2015connection}. Add to this a lack of routine reporting of interpretable effect sizes, fitting of many models without adequate explanation, and poor data and code reporting habits all increase the chance of finding `significance' at a level of $\le0.05$ \citep{halsey2015,loken2017}. 

The answer to these problems is not to make $p$-values smaller \citep{halsey2015,colquhoun2017}, nor is it Bayesian, machine learning or `new' causal inference approaches, despite assertions to the contrary, which echo previous promised revolutions through a new method \citep[e.g.,][]{mitchell1992testing,Burnham2004,byrnes2025causal}. % which (as we touch on above), bring their own ways to sift `exciting results' from what is actually a pile of chafe. %  (\href{https://www.youtube.com/watch?v=c3hxhv0lpI0}{`I am arbitrary but my story is often told ... '}) % 
Many fields are increasingly using machine learning models to guide science, but they can easily lead to poor models that do not match underlying realities of the system \citep{efron2020prediction,pichler2023machine}. Ecologists have readily taken up such approaches, in hopes they will help them fit better models. Similarly the field readily took up path analysis, model comparison with AIC, and a suite of other approaches, that promised better inference given mastery of one new approach that led eventually to hundreds of papers reporting poor models, and resulting policy recommendations based on such models \citep{petraitis1996inferring,leroux2019prevalence}. This fad approach to statistics is not unique to ecology, but the cure for it is also not a new statistical approach.

We argue that the answer is training in workflows designed for careful model building, model fitting and model interrogation informed by underlying theory and understanding \citep{betanworkflow,gelman2020bayesian,vandeschoot2021}---including the one we outline here. This workflow depends strongly on simulating data---for testing your model (Step 2), and understanding your model results (Step 4)---an area we actively under-train in many research fields that depend on increasingly complex statistical methods. Simulation approaches encourage interactive learning, build intuition, and stress exploring a model in its relevant context. Ecologists, similar to researchers in any domain-specific field, are much better at thinking about domain-specific scientific problems than statistical ones. Grounding statistical approaches in theory and understanding will likely bring the best out of statistical modeling. While this idea is not new, we argue the need for it is especially high, as the line between estimation and prediction becomes more blurred \citep{shmueli2010explain}. At the same time, however, computation is increasingly part of a researcher's toolkit making the opportunity to widely adopt this workflow highly possible. 

% As larger datasets and machine learning increase their utility, and prevalence, methods to test our understanding, interrogate our models and develop new models, will depend strongly on simulation, potentially transforming ecology, as it is transforming science in general \citep{flynn2022digitaltwin,kuntz2022,oren2017}. The workflow we outline here thus applies to many other statistical realms, including machine learning, resampling and traditional null-hypothesis testing using frequentist approaches.

\emph{Acknowledgements:} Comments from F. Baumgarten, B. Bolker, D. Loughnan, N. Pates, V. Rudolf and V. Van der Meersch improved this manuscript and J. Ngo improved Figure 1. Thanks to V. Van der Meersch also for helping with Markdown formatting. % S. Schmitt  from NEE review

\iffalse
\subsection*{Conclusions}
\begin{enumerate}
\item Ecologists cannot simulate their stats (or simple systems for that matter). Evolutionary biologists can. (And the field is better for it.)
\item Maybe hint at that you need these skills (and unit testing) given rise of AI?
\end{enumerate}


{\bf Take home messages (maybe)}
\begin{enumerate}
\item You should not fit a model you cannot simulate
\item Fit simpler models
\item Know your nonidentifiability
\end{enumerate}
\fi 

\newpage
\section*{References}
\vspace{-5ex}
\bibliography{refs/bayesrefsmini.bib}

\clearpage

\section*{Tables \& Figures}
% Words to define in table/glossary
% Classical approaches/methods. 

\begin{table}
\caption{Glossary: We provide below simplified definitions of the major terms we use (many of these terms, such as calibration, may be used differently depending on the particular literature). }
\begin{tabular}{ p{3 cm}  p{13 cm} }  \hline \hline
 \emph{Term}   & \emph{Definition}\\ 
\hline \hline
calibration & analyzing how often an estimate is close to the true value over an ensemble of hypothetical observations.  An exact calibration would requires simulating from the true data generating process which is impossible in practice.  We can, however, calibrate to data simulated from the configurations of models we plan use to fit to our data (\emph{Steps 1-2}) so we understand the models better, including their limits given data similar to ours. We emphasize simulations to calibrate model behaviors consistent with our ecological systems and understanding (e.g. working within a limited set of parameter ranges through prior predictive checks). In contrast to this approach, frequentist method are calibrated against all possible behaviors, which is not only impractical for complicated models it’s also irrelevant given that the most extreme behaviors are unlikely to manifest in reality. \\\hline
degeneracy & complex uncertainties that come from a mix of sources, including, non-identified models and cases where the data cannot well inform model parameters. When the data are not informing the parameters that we care about, this highlights a measurement issue. Identifying these problems in simulation studies can highlight when we need a better experimental design (e.g. sampling for more overlapping species across sites, or changing what we measure, etc.).  \\\hline
non-identifiability & when all parameters in a model cannot be uniquely identified with infinite data \\\hline
prior & an distribution of reasonable values for a parameter based on fundamental biological and ecological understanding, previous research, or other sources \\\hline
statistical model & Mathematical approximations of the true data generating process labeled with numerical parameters.  Evaluating a statistical model on observed data gives a likelihood function that quantifies how compatible different parameters are with the observed data, and hence can be used to `fit' the best parameters. In this article, we often simplify to `model.' See also the Supplement: What's a model? \\\hline
posterior & product of the likelihood and prior; that is, a probability distribution that quantifies how compatible different model parameters are with both the observed data and the domain expertise encoded in the prior model. \\\hline
workflow & a set of steps to achieve a goal, with those steps designed to help organize the process, and ideally make it more systematic  \\\hline
\hline
\end{tabular}
\label{tab:glossary}
\end{table}


\newpage

\begin{figure}[ht]
\centering
\noindent \includegraphics[width=1\textwidth]{figures/workflow.png}
\caption{The four-step iterative workflow we outline can help design models for specific ecological questions, data and aims---which makes this a statistical workflow that can naturally become a scientific workflow. It makes the step that many of us focus on---running your model on your empirical data (Step 3)---far more straightforward and insightful by using simulations both before (Step 2) and after (Step 4) it to better understand the model and data together.}
\label{fig:workflow}
\end{figure}

\begin{figure}[ht]
\centering
\noindent \includegraphics[width=0.9\textwidth]{figures/twoexamples.png}
\caption{We provide two very simple examples of the first steps of this workflow as Supplements (in PDF from R Markdown files). One example uses ordinary least squares regression considering a natural experiment on lizards on tropical islands, and simulating two different possible sample size. The other example uses lasso regression to examine how environmental variables may predict tree leafout. See supplements: `Is a sample size of five stormy islands enough?' and `Identifying predictors of tree leafout.'}
\label{fig:twoexamples}
\end{figure}

\begin{figure}[ht]
\centering
\noindent \includegraphics[width=0.95\textwidth]{examples/synchrony/graphs/fourpanelforpaper.pdf} % from 1000 sim. datasets (purple) \n versus empirical data (pink)
\caption{Example of retrodictive checks (Step 4) and feedbacks from time-series data of phenological events over time. The empirical data (top left, pink) looks similar to one simulated dataset (top right, purple), based on existing species number, their respective $x$ data, and simulating from the parameters for each species, but the spread of the simulated data seems possible higher. Repeating this retrodictive (or posterior predictive) check 1000 times, and taking the standard deviation (SD) of each simulated dataset, and looking at the resulting histogram confirms this (lower left in purple, empirical data SD in pink). This leads to an updated model, where the same retrodictive check looks closer to the empirical data (lower right), but still could be improved. These examples are shown in full in `Steps 1-4 in a Bayesian framework' in the Supplement and available at: \url{https://github.com/lizzieinvancouver/bayesianflowsexample}.}
\label{fig:retrodictivecheck}
\end{figure}

\end{document}

