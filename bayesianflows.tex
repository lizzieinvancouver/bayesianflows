\documentclass[11pt]{article}
\usepackage[top=1.00in, bottom=1.0in, left=1in, right=1in]{geometry}
\renewcommand{\baselinestretch}{1.1}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{gensymb}
\usepackage{xcolor}
\usepackage{xr-hyper}
\externaldocument{bayesianflowsysupp}
\usepackage{hyperref}

\def\labelitemi{--}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead[LO]{}
\fancyhead[RO]{}

\begin{document}
\bibliographystyle{/Users/Lizzie/Documents/EndnoteRelated/Bibtex/styles/besjournals}
\renewcommand{\refname}{\CHead{}}

%%%%%%%
%% To do %%
%%%%%%%

% Test data code for y = ax + b?
% Ask WDP to add prior rants
% https://github.com/lizzieinvancouver/gelmanhill/wiki/Vocabulary
% TODO below


\title{A four-step Bayesian workflow for improving ecological science}
% Simulation as a best practice in Bayesian workflows and beyond
% How to fit Bayesian models and influence people
% How to do Bayesian model fitting in ecology 
% The best way to be a Bayesian in ecology today
\date{\today}
\author{EM Wolkovich, TJ Davies, WD Pearse \& M Betancourt}
\maketitle

% 160 words (200 max for NEE); main text is 4,000; 3-4K for a perspective, see https://www.nature.com/natecolevol/content
\abstract{Improvements in algorithms and computational speed have heralded a new era of Bayesian model fitting. Models are easier to fit, faster to run, and more flexible, making them ideal for capturing many of the complexities of sophisticated sampling designs and bigger, messier datasets. This has opened up Bayesian approaches to a new world of users. Yet many ecologists do not currently take advantage of these new possibilities, and instead rely on similar approaches, work-arounds, and diagnostics that served them well for traditional methods centered on null hypothesis testing. Here we describe a broadly generalizable workflow for Bayesian analysis centered on simulating data from models, and show how it can revolutionize training in ecology.  This approach represents a ground shift of not just how to fit Bayesian models, but how we should approach model fitting to advance our science and builds on the increasingly computational toolkit of many ecologists.  By integrating model building and testing more fully with ecological theory, concepts and understanding this simulation-based workflow helps fit models that are more robust and well-suited to provide new ecological insights. This in turn can refine help us where to put resources for better estimates, better models, and better forecasts. While we outline this workflow using Bayesian methods, the general approach could be used by anyone fitting models to ecological data.
 % Steps of this workflow will often highlight current limits of our inference and suggest paths forward, as they provide a clear method to test how uncertain models are given small sample sizes, or how often a poor model may appear `best' given certain diagnostics. 
% Maybe along the way we highlight common practices in ecology that are stupid as all tomorrow---as our approach makes clear---and outline best practices for jumping into the wonderful surf of the happy Bayesian ocean. %JD16Aug: Obvious bad practices: small sample sizes and forking paths, thresh-holding on p-values, model selection and minimum adequate models etc.
% OLD version of second half (including our Bayesian beach): Here we present a Bayesian workflow---centered on simulating data from models---that represents a ground shift of not just how to fit Bayesian models, but how we should approach model fitting to advance our science. This workflow integrates mechanistic and statistical models with a computational toolkit, which we argue can accelerate ecological science. By interrogating our methods through generative models, we can refine where to put resources for better estimates, better models, and better forecasts. While we outline these methods from the happy surf of our Bayesian beach, the ocean we describe works for anyone fitting models to ecological data.

\section{Main text}
\setlength{\parindent}{0pt}
\setlength{\parskip}{7pt}

% 1Nov2023: Shorten and/or move to end?
Recent years have seen the explosion of Bayesian models across scientific fields \citep{vandeschoot2021,schad2021,grinsztajn2021}, including ecology. This change comes in part from increased computational power, but more from new algorithms \citep[e.g. Hamiltonian Monte Carlo,][]{nuts2014,betan2019} that have made fitting and implementing Bayesian models faster, more robust and arguably easier \citep{Carpenter:2017stan}. Capitalizing on these advances \textsf{R} packages, such as \textsf{brms}, that streamline fitting a set of pre-determined models have seen growing use. Bayesian approaches, long heralded as more powerful, flexible and especially adept at capturing the multiple levels of variance in ecological data, are positioned to advance ecology as a discipline. % They could also steer it away from null hypothesis testing, and its related focus on $p$-values, which have led to replication crises in other fields. % Bayesian approaches, long heralded as more powerful, flexible and especially adept at capturing the multiple levels of variance in ecological data, seem poised to advance ecology as a discipline, and to avoid contributing to the replication crisis.

% Capitalizing on these advances \textsf{R} packages, such as \textsf{brms}, have seen much wider use than the previous generation of packages that streamlined fitting a set of pre-determined models using Bayesian approaches (e.g. \textsf{MCMCglmm}). We believe that Bayesian approaches, long heralded as more powerful, flexible and especially adept at capturing the multiple levels of variance in ecological data, are positioned to advance ecology as a discipline.
 
At the same time, ecology itself appears well positioned to apply a century of theory and empirical insights to address the global challenges society is facing. Growing anthropogenic challenges have increased the need for predictive models from ecologists, as policy-makers demand forecasts of how communities and ecosystems will shift with climate change, habitat degradation, and other anthropogenic forces to balance conflicting societal needs and desires. The solutions ecologists can offer are rarely simple, and communicating the inherent risks and trade-offs in decision-making would benefit from forecasts that include uncertainty, and highlight the variability of ecological systems that is often as, or more important, than means or other summaries. % WDP: The above makes me think that we do need to get the whole marginal probabilities stuff into the text below
% [JD adds: models that embrace uncertainty, something we still find hard to communicate well, and variability vs mean responses...]

To meet these demands, ecology has developed ever larger datasets to test fundamental theory across systems \citep{Hampton2013}. As ecological data has grown in size, however, so has its complexity. Bigger data is messier data. To make meaningful inferences from such data generally requires a model of not just the underlying biological processes, but also how the measurements were taken---what interruptions or various `demonic intrusions' \citep{Hurlbert:1984br} occurred. While some subdisciplines have long used Bayesian approaches for this complexity  \citep[generally in fields focused on inferring population sizes of things people want to eat or manage,][]{millar200,0muthuku2008,zheng2007,trijoulet2018,strinella2020potential}, most have continued to try to adapt traditional statistical methods (e.g., $F$ and $t$ tests) and continued to focus on null hypothesis testing. Yet the complexity of most large ecological datasets violates assumptions of many these methods, making them supremely fragile when used beyond the cleaner, simpler experiments they assume. For example spatial, temporal and phylogenetic correlations often violate the independence assumptions inherent to these methods.
% Ecologists have long been skilled at documenting the full process underlying their data---field notebooks are often filled with notes on potentially relevant observations (from small to large disturbances, to when certain insects appear in plant plots). Yet they have rarely been challenged to build models that include these complexities. 
% Early concerns have given way to a widespread appreciation of the value of these new approaches.
%  (alongside a resounding and bizarre fervor over `natural history'). EMW -- commenting out my rant... and now the whole sentence. 

Ecologists try to address some of these challenges for larger, more complex datasets by fitting multi-way interaction terms, using random effects to correct for group level factors, or comparing a cross of suite of models. But the outcomes are not often aligned with ultimate aims:  multi-way interaction terms can make main effects hard to interpret, null hypotheses tend to be rejected even when the underlying effect sizes are weak \citep{gelmanhill,muff2022rewriting}, and many model comparison approaches prefer models whose inferences match the idiosyncratic fluctuations in the ecological data, but donâ€™t generalize to other observations. These practical realities highlight that our current approaches are not meeting the challenges of our data combined with our ultimate aims in ecology today.

Today, as large-scale ecological data becomes available for more diverse systems and for questions addressing other aims, ecologists are using Bayesian models in new ways. Unfortunately, there is naturally a lag in training in statistics, and in Bayesian modeling in particular. Bayesian approaches provide a pathway to powerful models that can transform how we understand our systems, but they can also lead to pitfalls most ecologists are not trained to notice or manage. Many of these pitfalls can be avoided by approaching Bayesian analyses through specific workflows \citep{betanworkflow,vandeschoot2021}, which themselves are built on a process of how to do not just statistics, but how to do science. % Maybe address workflow comment here? 

Here we describe a broadly generalizable workflow (see Table \ref{tab:glossary}) for Bayesian analysis and show how it can revolutionize training in ecology. By integrating model building and understanding more fully with ecological theory, concepts and understanding---and vice versa---this approach can fit models more robust and well-suited to building on providing new ecological insights than traditional approaches. Once you start doing this workflow, your scientific life will never be the same. 
%  allow fitting complex models without much influence of the data fed into the model. 
% show how it can revolutionize training in ecology by integrating model building and understanding more fully with ecological theory, concepts and understanding.
% 

\subsection{A short guide to statistical workflows and Bayesian approaches}
% EMW: Not sure on my use of the word 'estimate'
Statistical analyses are designed for inference---to learn about some process or effect or behavior from data  (see Table \ref{tab:glossary}). Robust analyses, however, rely on our inferences being consistent with the underlying truth more often than not.  Quantifying this consistency is calibration, a critical part of using models for inference. A major problem with traditional (frequentist) approaches in ecology today is their inferences are unpredictable when their foundational assumptions fail, but ecologists are not usually trained in how to recognize or deal with this. In contrast, the workflow we recommend avoids these pitfalls. It provides an organized sequence of useful steps to explicitly calibrate a model by studying its potential inferential behaviors and prepare for their consequences. We believe it's much easier and straightforward to do with a Bayesian approach and thus outline the workflow with that approach in mind. % Most ecologists are trained primarily in frequentist methods, which are focused on calibration (this is is why training in frequentist methods includes so much discussion of significance, power, mean squared error, and the like). Bayesian methods typically focus on inference, but can be equally well calibrated (see Step 1, below). 

To better understand our workflow, we provide a very brief overview---inherently incomplete and, by design, not very technical---of some of the fundamentals of Bayesian methods. This section can be skipped for those who feel already well-versed, and can be augmented for those who are new to Bayesian approaches \citep[for example,][]{statrethink,BDA,regotherstories}.

Many of us have some intuition as to what a probability is, and often it is ``the long-term frequency with which something happens.'' We would expect, for example that if we tossed a coin 100 times we would see roughly 50 heads.  In this case we would say that the probability of tossing a coin and getting a `head' is $\frac{50}{100}$, equivalent to 50\% or $\frac{1}{2}$. At the same time we wouldnâ€™t be very surprised if we observed 49 or even 55 heads, although we would be surprised if we saw 99.

This definition of probability---which is the \emph{frequentist} definition---is useful in many situations, but it has a few disadvantages. First, frequentist definitions aren't very useful when dealing with totally unexpected situations. Frequentist probabilities are grounded in repeatable observations. Understanding these repeatable frequencies is of little use when trying to make predictions for changing systems. Second, frequentist calibration is sufficiently challenging that most common frequentist approaches rely on specific assumptions. Properly using frequentist statistics requires trying to match a given mechanistic model and dataset to a frequentist method where the assumptions are most closely met. Given the complexity of ecological data and our uncertainty about the underlying model, these disadvantages of frequentist approaches can be especially challenging in ecology. It would be nice to have a statistical way to build models that can propagate our (un)certainties about we do---or especially, don't---know into our statistical modelling, and that we can calibrate and understand a model's inferential behaviour given assumptions consistent with what we know about how ecological systems work. % Second, frequentist calibration is sufficiently challenging that most common frequentist approaches rely on analytical solutions, which---given the difficulty of such solutions---are based on simplified models with specific assumptions

Luckily there is an alternative definition of probability that allows us to incorporate so-called \emph{belief} into our definition of probability: Bayesian probability. The Bayesian definition of probability is:

\begin{equation}
  probability = \frac{likelihood \cdot prior}{normaliser}
  \label{bayes_theorem}
\end{equation}

Where $probability$ is the Bayesian probability, $likelihood$ is exactly the same as a frequentist likelihood, \texttt{prior} is your best-guess of the probability (your `belief prior' to collecting data), and \texttt{normaliser} is a mathematical constant that makes sure our probability cannot go above 100\% or below 0\% (statisticians are lazy, and will not `give 110\%'). This mathematical constant (technically it is the probability of our data; $p(data)$) is a nuisance term that is extremely challenging to estimate (sometimes it is impossible!) and held back the practical application of Bayesian statistics for many years. Nowadays, with increases in computer power, we can use numerical methods such as `Markov Chain Monte Carlo' (MCMC) methods to avoid having to estimate its value precisely. Such methods are simulations  that involve chance at each step (that the process proceeds in iterations or steps, and only the last iteration affects the next, is why this is a `Markov Chain'), tweaking and changing coefficients within your model until a distribution of plausible coefficients are found. This distribution is called the `posterior' distribution to distinguish how it is our view of probability \emph{after} the data and the prior have been considered. %There is no guarantee that the posterior distribution will, itself, be found: the user must check for so-called `convergence' onto the true posterior distribution as the process involves randomness. It is for this randomness that `Monte Carlo' is added into the name, after a city famous for gambling.

% It turns out there is a further advantage that such an approach gives us. While frequentist probabilities are all about the probabilities of observing data ($p(data|model)$: what are the chances of getting 99 heads in 100 coin-tosses?), Bayesian probabilities are all about the probabilities of observing models ($p(model|data)$: what are the chances this coin is biased towards heads?). This makes it straightforward to report the results of Bayesian analyses in terms of estimates of uncertainty and precision of coefficients (contrast ``I'm 80\% certain that coin comes up heads at least two-thirds of the time'' with ``I'm less than 5\% certain those last twenty coin tosses came from a fair coin'').

% WDP: I started trying to write about how you can do conditional probabilities using posterior distributions but I realised that I'm beginning to stray into the benefits and pitfalls section below. 

\subsection{A brief overview of the benefits---and pitfalls---of Bayesian models} 

Bayesian models have many benefits, but an often-mentioned one is that `you can fit any model you want.' While this is not entirely true \citep{BDA,reid2019}, Bayesian modeling options can feel limitless when compared to the models ecologists can fit (with convergence) in popular modeling packages (e.g. \textsf{lme4}). As long as you can write out the likelihood of your desired model and assign priors to all parameters, you can generally `fit' any model. This includes non-linear ones, non-Gaussian families (e.g. Poisson, beta or combinations thereof, such as hurdle models), hierarchical designs and any combination of these, as well as `joint' models where parameters estimated in one equation appear in another, propagating uncertainty. Such flexibility is incredibly powerful in ecology where data are often influenced by complex spatial or temporal patterns, non-linear processes are widespread, and common data types are non-Gaussian (e.g. counts, percent cover etc.). 

Fitting a bespoke model to data also yields numbers we often really want but don't have access to in other approaches. We perform experiments because we want to know how a treatment affects something of interest---how much slower do birds wearing backpacks fly, or how much faster to plants grow with more warmth---but we often become more focused on whether the treatment was `significant' or not. We lose track of whether birds flew 15\% or 50\% slower, and how consistent the effect was across individuals. But models can be designed to estimate and report effects per mg of backpack weight, or per \degree C of warming---always with estimated uncertainty. While replication crises in other fields, driven in part by a overly zealous focus on $p$-values \citep{halsey2015,loken2017}, and the rise of meta-analyses in ecology \citep{Hampton2013} have led to a somewhat greater focus on `effect sizes' in ecology (often used to refer to very specific unitless statistics, such as Cohen's $d$), bespoke models take this to a new level. Researchers can easily estimate comparable effect sizes from $z$-scored data (in units of standard deviation), alongside estimates in meaningful natural units, such as per \degree C of warming or per hectare of habitat lost.

% WDP: Below is a point that I think follows from what you have above; it's also something I'm keen on, as I think I've banged on about by now. Apologies because this text is a little rough; hopefully the spirit is coming through though.
Further, using Bayesian methods makes it easy to report estimates of probabilities about coefficients (\emph{e.g.}, ``with anthropogenic warming, we are 80\% certain the flowers open earlier'') rather than null models (\emph{e.g.}, ``we are less than 5\% certain these data came from a model where flowers aren't responding' to anthropogenic warming'). Relatedly, frequentist tests are based on accepted level of Type I error rates---a particular $\alpha_{crit}$ (often 5\%), which is often confused with statistical power ($\beta$, the probability that we would detect a true effect): when we define significance as a less than 5\% chance that some data came from a null hypothesis, it does not mean we have a 95\% chance of detecting a true effect \citep[indeed statistical power is often extremely low in ecological studies,][]{jennions2003survey}. Bayesian statistics, by allowing us to compare the probabilities that \emph{models} are correct, and not the probabilities that \emph{data} are taken from a give model, allows us to test what we have always cared about: the biological processes themselves.

This valuable flexibility is often mentioned as one of the greatest pitfalls of Bayesian models: you can fit almost whatever you want, but critical parts of your model might be almost entirely unimpacted by your data. In ecological model fitting, we're currently most often interested in parameter estimates strongly informed by our data, making this problem sound especially dangerous. In reality, however, (1) this problem is not related to modeling, but to experimental design---and a flawed experimental design leading to low power for your model is much easier to see through this workflow compared to using traditional null hypothesis testing methods. (2) Such models are not as common as some may suggest. Perhaps more dangerous is fitting mis-specified models, where the model is not doing exactly what we think, either due to coding errors or poor understanding of the model. 

These `pitfalls' of Bayesian are not new, not necessarily specific to Bayesian methods \citep{low2014rising}, nor unique to ecology---though the complexity of ecological data and processes may make it especially pernicious in ecology---and decades of statistical research has aimed to develop best practices when using Bayesian models to avoid this. These best practices generally center around a specific workflow; a variety of which can be found in exquisite detail elsewhere \citep{betanworkflow,grinsztajn2021,vandeschoot2021}. We do not aim to repeat those here, but instead to provide a highly simplified---but powerful---workflow that, when applied to Bayesian modeling in ecology, could greatly accelerate progress. % This is a short overview of a much deeper topic. See \href{https://www.nature.com/articles/s43586-020-00001-2}{Bayesian statistics and modelling} for a little more depth, including how to develop priors and your basic model formulation.\\

\subsection{A very basic Bayesian workflow}

We outline a workflow below that includes what we consider the major steps for Bayesian model fitting (Fig. \ref{fig:workflow}), designed for those who aim is to build and develop a model for
your question/system/data. The workflow includes model calibration (Steps 1-2), inference (Step 3) and then model development (Step 4). Many of these steps will be familiar to statistical ecologists, but are often overlooked, whereas other steps may appear particular to Bayesian (e.g. prior predictive checks), but are actually useful for anyone---using Bayesian models or not---to challenge their models of how the world works. Parts of this workflow could be dropped, or expanded as workflows in themselves, given other aims (see Supplement: Which workflow?). 

As we focus on a simplified list of major steps, many of the smaller but still critical steps are omitted. For example, visualization is required at every step---especially 2 and 4; while we do not discuss this explicitly we refer to relevant publications. We also jump in, and somewhat over, one of the biggest steps. 

We present the workflow assuming users have a model already in hand, which---if you have collected data---is true. Your model may be only verbal or conceptual; however, for this workflow, you'll need to convert such models into mathematical versions. This can be challenging at first, as ecologists often learn only a simplified version of this step (often focused on identifying which distribution---normal, binomial, zero-inflated Poisson, etc.---their response variable most closely resembles), and approach it after data collection. In reality, skilled users of this workflow will develop their model before they collect and data refine it---or develop possible additions to it---as they collect data; models often fit best when they include both the underlying biological model being studied and a model of the data generating process (e.g., gaps between sampling dates, biases in sampling etc.). 

Getting to the point where the Bayesian workflow is part of data design and collection, however, requires starting somewhere---with some model in hand. A suite of resources o `generative' or `narratively generative' modeling can help \citep{statrethink,betangen}, along with two points. First, you must start somewhere, so know that you can and will improve on this skill. Second, as you start, ask lots of questions---and push yourself on your answers---what do you expect and what's reasonable biologically from your model? For example, instead of simply identifying to which distribution your response variable looks most similar, ask yourself what generates that distribution and what you think its mean, minimum and maximum is. Do you expect data below zero? Up to infinity? If not, why not? You'll be generating your model---including its priors---as you do this. Effective model building is about efficient brainstorming and this is a critical part of the process (see supp). 

Our workflow is explained mostly program-agnostically. Though at times we assume a user of \textsf{Stan}, a relatively new probabalistic programming language, that interfaces with \textsf{R, Python, Julia} (and more) to write bespoke Bayesian models and underpins the \textsf{R} packages \textsf{brms} and \textsf{rstanarm}, which fit a suite of specific (pre-defined) models \citep{Carpenter:2017stan}. We focus on \textsf{Stan} as its MCMC algorithm (a variant of Hamiltonian Monte Carlo, HMC) is fast and produces specific output to warn of model fit issues (i.e., divergent transitions) in a way other MCMC algorithms do not (e.g. Metropolis-Hastings or GIBBS), but the basic workflow should apply to diverse implementations of Bayesian modeling. \\

\emph{Step 1: Check your model code.} \\ 
% This obvious step assumes a suite of crucial work already done to define your model of interest. You must understand your question, your subdiscipline, and your data enough to formulate a useful model that you want to fit. This step thus assumes you arrive with a wealth of scientific expertise and a well thought-out model. 
To start the workflow, you need to write up your model and check it. As with all code, just because it runs, does not mean it does what you think it does. Whether writing it out in \textsf{Stan}, where you need to be able to write out the full likelihood and set all your own priors, or using a package that writes much of the model for you (e.g. \textsf{rstanarm}), you need the code and a way to verify the code is correct---test data.

Test data (aka `simulated data', or  `fake data,' etc.), and the skills required to build it, are central to this workflow. With `test data' you simulate data from your model in such a way that you can use the resulting data to test your model code is correct. This means that to build test data you need to understand your model well enough to generate data from known parameters; you then run that data through your model and confirm it returns the parameters (i.e., you fix values for your model parameters, then test how well your model recovers them). While there's no guarantee that inferences will always recover the parameter values you set, extreme disagreement is usually a good indicator that something is amiss. At the same time these simulations studies can help us calibrate how often our inferential method might recover the truth, and can be easily adapted into formal tests of power. As you do this, you will also be calibrating your model---seeing how close it estimates parameters you set and under what conditions. %  As the goal is to test your model code, you'll want test data that is well-replicated, balanced and otherwise ... (err, actually -- skip this -- as it assumes a model without a data generating process in some ways). 


This very basic model checking step is uncommon for many ecologists, but critical in our view. If you can simulate data from your model, then you can powerfully---and easily---answer questions related to statistical power, what effect sizes are reasonable, and---most likely---have new insights into how your model suggests the world works. `All models are wrong; some models are useful,' becomes much clearer when you have the power to generate data from your model under any parameter set and sample size you want. Conversely, if cannot complete this step, you'll struggle to understand if the model fit well, and struggle further to meaningfully interpret the model output, making this apparently simple programmatic step actually encapsulate a far deeper understanding of your model. Once your \textsf{Stan} output returns the parameters you expect from your test data, you can move onto interrogating your priors. 

\emph{Step 2: Check your priors.} \\
Priors are the source of much discussion and focus by Bayesian and non-Bayesian researchers alike. Often treated as the big bad wolf of Bayesian, or the unseen hand producing the model fits you get, according to some. They show up as half of the equation that gives you your model posterior, and philosophically, Bayesian was built around the idea that you have prior knowledge that you trust and want to compare to new data (with your new data showing up through your likelihood). In reality, few Bayesian analyses in ecology are approached this way. 
%JD16Aug --  Dolph always compares the 'danger' of priors in the belief that Saddam Hussein had WMDs - data was sparse, so priors overwhelmed. Han Solo flying the Millennium Falcon through an asteroid field illustrates the benefits of priors. We need to know where we are on the Saddam - Han spectrum.
% WDP: Jonathan, this is the first time I've read you making a sci-fi reference in print and I'm extremely excited by it

How much priors influence your model fit is up to your model and your data. Depending on those two parts, the likelihood (influenced by your data) can easily overwhelm your priors. Indeed, most work on the dangers of priors and `prior misspecification'  focuses on cases where you have very little data for the model you're trying to fit. If you try to fit a Bayesian model with 100 parameters and only 5 datapoints, then your priors will likely matter a lot. Priors can also matter when you have fewer parameters and lots of data but the data are not informative for the model; for example, if you fit a model including parameters for estimating high temperature responses, but lack data at those temperatures. Priors, however, can only matter more than you know when you fail to think through and check them. 
% In our experience, however, the most dangerous part of priors is not that they will matter more than you know, it's that in failing to think through them and check them, then you'll fail to understand your model enough, and be unprepared to interpret its output. 
% \colorbox{pink}{[Refer to your rant paper here?]} -- and below

Assigning priors generally forces you to think about your model with regards to your study system, and interrogate what's probable, possible or actually unreasonable. While many packages (e.g., \textsf{brms, rstanarm}) will automatically set default priors, assigning them yourself (which you have to do if you write your own code) can quickly disabuse newbies to Bayesian and skeptics of their prejudices. For example, you may not think you have a prior on how sunlight affects plant growth, until you realize your `agnostic prior' actually allows plants to grow hundreds of meters per day. % something like `and generally this will not impact your results, see Supp'

You can take this a step up with prior predictive checks, which serve both as a check on the priors you're using, and to further explore the model of the world to which you're planning to fit your data. In prior predictive checks (which can easily be done by adapting your code from Step 1), you explore a distribution of potentially reasonable values for your priors, then see how they influence your resulting output. How exactly to do this depends on your question, model and aims, but many guides can help you think through this \citep{betanprior,wesner2021,winter2023}. 

By examining the consequences of given prior model you may suddenly realize that prior values that previously seemed reasonable lead to heavily unrealistic results when embedded in your full model, with all the other priors on parameters. You may find you have set up a model where certain effect sizes mean birds fly backwards when given heavy-enough backpacks. This means both that you may want to adjust your priors, but also gives important insight into the practical
consequences of certain parameter configurations. Thus, if you see them in your actual model output you'll be more likely to realize your model is problematic (Step 4 will also help with this).

 \emph{Step 3: Run your model on your empirical data.} \\
The next step is to run the model---you've now validated, test-run and have ready to go---on your exciting new empirical data. Check diagnostics so you know it's running well and adjust until it is; this includes a suite of convergence and efficiency metrics ($\hat{R}$, ESS, lack of divergent transitions etc.) that are well-discussed elsewhere \citep[and not our focus here, see instead][]{betanworkflow,gelman2020bayesian,vandeschoot2021,gabryvis}
 
This is the step many ecologists skip straight to, ourselves included. It's easy to see the appeal---this is the inference step! Fitting our new data to the model can feel like the moment when we'll learn something new. But, at least in our experience, this is not always the case. When we rush to this step, that first model we fit is often followed by another, and another---perhaps because one does not converge, or the results of another do not make immediate sense. After a while of this process, it can easily feel like we're not sure what we learned, if anything. And we can get distracted from what we are actually most interested in---the inference into biology. In contrast, by approaching the model through Steps 1-2, it's often much easier to quickly see through the results of the model fit. And also easier to plan next steps. 
 
% This is the step many ecologists skip straight to, leaping over steps 1-2 without realizing all they are missing along the way. While we can understand this approach---given the excitement of wanting to see how new data influences the model---we hope it's now obvious how much more can be gleaned from data by approaching the model through Steps 1-2. 
%  As we're all familiar with this step, it's hopefully straightforward. Run the model---you've now validated, test-run and have ready to go---on your exciting new empirical data. Check diagnostics so you know it's running well (convergence metrics, lack of divergent transitions etc.) and tweak until it is. 

\emph{Step 4: Posterior retrodictive checks} \\
% Once you have your posterior based on your model and new empirical data, it's time to interrogate it. 
Once you have your posterior based on your model and new empirical data, it's time to remember that it's wrong (`all models are wrong...') and ask how useful it is. Step 4 This step is where you define what a model needs to be useful and then check if it achieves that goal. Just because your model gave you estimates doesn't mean the model is adequate for the data or your particular aim, and---depending on why you fit the model---you may especially want to make sure it is reasonably predictive. You can do some of this through diagnostics, such as $R^2$, which compares compares point predictions to the observe data, but with a posterior you can compare an entire distribution of predictions to the observed data. This is where simulating from your model can be especially insightful. Steps 1-2 have set you up well for this, as you have a sense of what different parameter estimates do to the model, and you've hopefully calibrated your model to have a sense of how it works on data similar to yours.
% you may especially want to make sure it is reasonably predictive. You can do some of this through diagnostics, such as $R^2$ and examining posteriors, but simulating from your model can be especially insightful. Steps 1-2 have set you up well for this, as you have a sense of what different parameter estimates do to the model, and you've hopefully calibrated your model to have a sense of how it works on data similar to yours.

Now that you have the parameter estimates from your posterior you can simulate new data from them and see how that new world looks to you---called posterior retrodictive checks (or posterior predictive checks, Fig. \ref{fig:retrodictivecheck}-\ref{fig:retrodictivecheckSD}). Exactly how to do these are---again---dependent on your question, model and aims \citep[but there is lots written on this,][]{held2010,gelman200ppc,conn2018}. In contrast to prior predictive checks, however, this step is built into some software. If you use \textsf{rstanarm}, then the package \textsf{shinystan} will automatically give you a set of posterior retrodictive checks, including comparisons of the mean and variance of simulated datasets with those of from your empirical data. 

% TO DO: I think that the last paragraph would benefit from a sentence emphasizing interpretability.  If we understand what parts of a model influence the particular data feature we're visualizing then we can hypothesize what might be inadequate about the model.
Often here you may find big differences from your empirical data, and can start to generate hypotheses for why. For example, you may find patterns that suggest missing grouping factors (e.g., site or biome) through visualization, and by grouping posteriors by that factor, or you may quickly realize your model predicts impossible numbers for your biological reality because of the distribution. 

\emph{Feedbacks \& workflows}\\
With those hypotheses in hand, you may very well want to tweak your model---this is all part of the workflow. At that point you return to Step 1, tweak your code, and repeat the process. In this way, fitting multiple models is encouraged, but in a far more structured and careful way than traditional ecological model fitting in our experience. This approach is different than the quest for a minimum adequate model or one `best' fit. Feedbacks in this workflow are focused far more on what is biologically reasonable, and understanding the utility---and limits---of inference from your data for your model.  And there are big benefits to it. 

This process more fully integrates mathematical modeling into statistical modeling. To complete Step 1, you have to understand the underlying math of your model enough to simulate data from it. This can be challenging at first (e.g. recalling how to simulate $y$ data for a simple linear regression is not straightforward when you never do it), but is immensely beneficial to forcing you to understand your model and its consequences Indeed, we have found the greatest insights come not from the step we all know best---fitting the model with empirical data---but from every other step in this workflow. Developing simulated data to test the model, running prior and retrodictive checks all dive you deep into understanding your statistical model, which suddenly you may find yourself thinking through much more mechanistically. In our experience this process has quickly translated into insights for our biological systems, and changed how we approach statistical models. % MB: I wonder if it's beneficial to frame this more as "dive deep into connecting your statistical model to your domain expertise"?  I often find myself trying to emphasize that people already have the domain expertise but they're not incorporating it into their analyses.  A lot of what this workflow is trying to do is facilitate that integration to leverage _what the analysts have always had!


This workflow represents just one of many possible workflows. It organizes a simplified set of steps for model calibration, inference and development---all implemented by simulating data from the model in various ways. Any one of the steps could be expanded into a workflow of its own (see Supplement: Which workflow?) and in some cases you may skip steps or need to expand or adjust them. No one workflow is ideal for all aims or everyone, and thus this workflow provides a possible template that should be adapted for different users, different aims, and should evolve as ecological/computational/statistical methods develop. 

\subsection{How this workflow changed our science} % Surprising things that happened to us that may happen to you if you follow this workflow
% JD16Aug -- I wasn't sure I was going to like this section, but you won me over - I just struggled with the 'simplify' section a little 

As we have used this workflow, how we approach our statistical models has changed. These changes have generally been similar for each of us. We suspect they are not unique to us, our study systems or questions. Instead, we think they represent common approaches to statistical modeling in ecology that could help the field advance, much as we believe they have helped our science advance. 

 \emph{Understanding nonidentifiability} \\
Identifiability and---more common in our experience: nonidentifiability---refers to when all parameters in a model can be uniquely identified with infinite data. Models can be nonidentifiable in several ways, including when mathematically some parameters cannot be uniquely defined. Statistically, a close kin of nonidentifiability---degeneracy---is often is an outcome of the empirical data combined with the model. Degeneracy occurs when the data do not contain ``sufficient information for unique estimation of a given parameter or set of parameters in a particular model'' \citep{gelmanhill}. 

Nonidentifiability can come up in many ways in ecology---and be hard to see, especially if you rush through model fitting. But if you have to write out your model and simulate data, you may suddenly realize lots of places for nonidentifiability and degenercies to live. For example, when species do not occur across most sites, a model including separate parameters for site and species is often degenerate, but there's no warning in packages to tell you this. Thus you may never realize what experimental designs and sampling regimes are fundamentally too imprecise for your questions of interest. We have become far better at noticing nonidentifiability and degenercies based on this workflow---and we have adjusted how we collect data and interpret results because of it. % JD16Aug -- and sometimes it doesn't matter if your are interested in the effect of a separate parameter.

 \emph{Know your limits} \\ % Dive into the simplified complexity conundrum
Once we noticed how pervasive nonidentifiability and degenercies were we started simplifying our models. Whereas previously when we had data that qualitatively appeared complexly nested, crossed, split or twisted, we would have tried to fit all of these intricacies (on the intercept) we are now more slow to add these to our models. By both understanding these terms better (including the many different (possibly redundant) ways each can be modeled), and understanding them better---depending on the data and model in each unique context---we work more carefully through what to include. % JD16Aug -- I feel there is a tension between the philosophy of 'if you think it might be important, then include it in your model' and make your model simple. We want both. But perhaps all those crazy interaction terms that are impossible to make sense of could be ignored ... 

% JD16Aug -- this was even referred to as 'model simplification' and we searched for MAMs (minimum adequate models) - See Crawley.
Before using this workflow, some of us would start with complex models, then simplify models until they converged in our given software package. Often these were hierarchical models with many levels---for example, including every column of site, plot, transect and quadrat in our dataframe, without stopping to check how well sampled they were, or what degeneracies they might introduce. These models were built based more on a dogma of non-independence (and, somewhat relatedly, correct degrees of freedom) than an understanding of the model and the system's ecology. Our approach was driven far more by a weak set of statistical assumptions constrained by our software package, rather than by our ecological understanding or ultimate aims. 

We also often fit a suite of interactions: multiple two-way interactions and the occasional three-way interaction were common fare. But in simulating data, and fitting models to real, messy, imbalanced data using the workflow we came to see how much we were asking of our data and models together. Fitting a two-way interaction with half the effect size of a main effect takes a 16X sample size, compared to fitting the main effects alone \citep[the main effects then average over the interactions, see][for more details]{regotherstories}. This is sobering in theory. It's more sobering when you see it played out again and again through this workflow. 

We now both add complexity and simplify based on a more careful reckoning. Usually our starting model is not simple, it often includes grouping factors that may be difficult to fit, but that we see as absolutely critical to the question, model and data at hand. We still may add and consider additional grouping factors and interactions, but we do so with a careful idea of how stable the model given the data likely is with them, and we rarely fit complex three-way interactions or similar---unless we have carefully designed the model and data collection for that aim. The ending model is often not as complex as we may have fit if we did not follow all the steps. And---through them, especially Step 4---we can feel confident our model is capturing important variation. 
% Once you do the workflow, you may end up like us: fitting fewer levels in your mixed effects models, fitting fewer interactions.

Understanding this variation includes re-approaching how we plot our model and data. While many of us are good at plotting raw data, and plotting our main model parameters (and plotting both these pieces together), the world in between is rarely taught. And it's in this in-between world that we have found a deep understanding of our models, what they're doing and---relatedly---what our results show. Many packages offer model predictions---based on a full or near-full model, but being able to decompose model predictions into varying components can give powerful insights. For example, model predictions can be plotted with and without major grouping factors such as `block,' `site', or `species,' and suddenly how predictable a model is (or not) based on other factors like `treatment' or `time' becomes clear. 

 \emph{Looking at parameters, not p-values} \\
Before this workflow, not all of us commonly discussed the values that parameters in our model took---things like the slope and intercept (two common model parameters) were sometimes reported, but we did not know them as well as we knew whether the $p$-value for the slope was $<0.05$. This changes quickly when you need to build simulated data, for example, for a phenological event (a day within the calendar year so, ideally between 1 and 365, or 366 in a leap year) and suddenly find it's $>$1000 based on the quick parameters you put into your code. 

This focus on the value of parameters scales up through this and other modeling workflows, and across projects. Having a better sense of parameter values across different biological contexts, model parameterizations, and time periods gives a better sense of how the biological world works, including what's reasonable, possible or wildly unrealistic. % Suddenly, weakly informative priors make ever more sense .... 

\subsection{How this workflow intersects with ecological training} %Challenges of the workflow compared to how we traditionally train ecologists

This four-step workflow is a simplified version of the current best practices for Bayesian model fitting  \citep{betanworkflow,vandeschoot2021}, but many of the skills required are not part of traditional ecological training. Writing out the math behind most statistical models enough to complete Step 1 bleeds into the skillset usually reserved for those working on theory, where coding and simulating from a model are common tasks. In contrast field, lab and otherwise empirical-data based ecologists often fit models they could not simulate data from. This dichotomy seems short-sighted in our current era of bigger, messier data and greater computational methods poised to handle such messy data---if scientists are trained in how to build the right models for their questions and data. % A reasonably competent coder could easily simulate data under a complex model that they might not have the mathematical expertise to solve analytically, if doing so was part of their training and the workflows they regularly use. 

A reasonably competent coder could easily simulate data under a complex model that they might not have the mathematical expertise to solve analytically, if doing so was part of their training and the workflows they regularly use. While simulation methods may appear foreign initially, they are usually much easier to implement than the analytical derivations of traditional methods so often seen in textbooks and classes. Further, simulations approaches provide clear routes to tests of statistical power (see Supplement), and stress exploring a model in its relevant---ecological---context. % We believe training ecologists with a skillset where they quickly can use this workflow could advance ecological science rapidly. % Theory = simpler models = outcome of a good Bayesian workflow (often)

Simulating data rapidly clarifies underlying assumptions. While training in frequentist methods often includes memorizing assumptions for a particular test, or steps specifically designed to test assumptions (e.g., normal quantile plots), this workflow requires no such training. Instead it requires only the skills to identify whatever the assumptions have been encoded in their
models. As such it moves away from some modeling paradigms in ecology, which focus on fewer underlying assumptions, to
building models were the assumptions are transparent and motivated by the specific domain expertise available in each
application. % Again, this is an area where those trained for theory often gain these skills, but it doesn't have to be that way, man.

The average ecologist today has a diversified skillset compared to an ecologist of decades ago---one that could potentially meet this challenge. Ecology has long been a field with deep statistical training, but the modern ecologist is now also expected to be computational: to handle large datasets, produce repeatable workflows, and translate models into forecasts for policy and planning. Many ecologists now bridge field, lab and computational methods. Bayesian methods can help facilitate this new job description, while also helping meet the demands for better models and forecasts. % As such, the rise of Bayesian approaches is especially timely for ecology. % The average modern ecologist is computational, but often grounded in the natural history of a particular system (err, maybe, on both these accounts, but whatever).

This workflow thus requires a shift in how we approach Bayesian methods---and training in Bayesian models---in ecology. Already, uptake of new Bayesian \textsf{R} packages highlight that Bayesian methods are no longer the purview of only wildlife and fisheries biologists, and these changes come alongside advances in Bayesian workflows, algorithms, and visualiziations \citep[e.g.][]{betanworkflow,vandeschoot2021,gabryvis}, that ecology must adapt its training for. While this is an active area, we highlight three major changes.

(1) Prior `beliefs' are changing. Best practices for determining priors is an active area of statistical research \citep{BDA,regotherstories,betanprior}, and training should reflect current best practices. These include that `non-informative priors' are a misnomer, as they are often informative  \citep{lemoine2019}, and priors can easily be `weakly informative.' Thus a strong focus on the dangers of priors in training can be overkill. % \colorbox{pink}{[Ref to rant?]}

Current training often includes a very strong focus on mathematically-convenient priors, because of the importance of conjugate priors in closed form solutions for particular posteriors. Modern algorithms, such as HMC, do not require conjugate priors, which are now antiquated. Prior predictive checks provide a far more powerful way to understand how priors work within a particular model, and are more useful than rules about which priors should be fit in certain cases or memorizing which priors are conjugate \citep{betanprior}. % (and thus sometimes referred to as  `the crystal deodorant of Bayesian statistics')

(2) `Random effects' are not just random. Hierarchical models contain grouping factors, sometimes referred to as `random effects,' such as species or individual. This term, however, is misleading, imprecise and thus no longer recommended \citep{gelmanhill}. In ecology, it also carries with a heavy weight of older `rules' of what is `random' versus `fixed,' including that `random effects are things you don't care about' (for example the `block' effect from a randomized block design). After posterior retrodictive checks (Step 4), you might feel differently, as hierarchical effects are (by definition) drawn from an underlying distribution---meaning you can predict outside of the specific set you sampled (for example, you can predict for a new species or individual), whereas you cannot do so for most categorical `fixed' effects.

(3) P-values, and null hypothesis testing in general, are easily misleading, and thereâ€™s no easy fix for that. The replication crisis, rampant in other fields, is based in part on an overly hopeful belief that $p$-values will separate the signal from the noise, with one easy number. In reality, small sample sizes, lack of routine reporting of interpretable effect sizes, fitting of many models without adequate explanation, poor data and code reporting habits all increase the chance of finding `significance' at a level of $\le0.05$ \citep{halsey2015,loken2017}. This reality means a similar crisis is likely lurking in ecology, especially given small sample sizes alongside a tendency to fit complicated models with multiple interactions. The answer to this, however, is not to make $p$-values smaller \citep{halsey2015,colquhoun2017}, nor is it Bayesian approaches, which (as we touch on above), bring their own ways to sift `exciting results' from what is actually a pile of chafe. %  (\href{https://www.youtube.com/watch?v=c3hxhv0lpI0}{`I am arbitrary but my story is often told ... '}) % 
% MB: Because the problem is null hypothesis testing, and not frequentist methods in particular, Bayesian null hypothesis methods like Bayes factors are going to be just as worse. Bayes factors don't seem to have infiltrated ecology yet, but it may be helpful to explicitly mention that.

The answer are workflows designed for careful model building, model fitting and model interrogation informed by ecological theory and understanding---including the one we outline here. %Try it, and see if it transforms your life they way it transformed ours! % Again I think it's worth being as specific as possible.  It's not just model building/fitting/interrogation it's model bespoke building/fitting/interrogation informed by relevant ecological domain expertise.
This workflow depends strongly on simulating data---for testing your model (Step 1), checking your priors (Step 2) and understanding your model results (Step 4)---an area we actively under-train in ecology. It makes clear how relevant and important simulation is, but the relevance of simulation extends well beyond Bayesian model fitting. As larger datasets and machine learning increase their utility, and prevalence, methods to test our understanding, interrogate our models and develop new models, will depend strongly on simulation, potentially transforming ecology, as it is transforming science in general \citep{flynn2022digitaltwin,kuntz2022,oren2017}.

\emph{Acknowledgements:} Comments from F. Baumgarten and D. Loughnan improved this manuscript. 

\iffalse
\subsection{Conclusions}
\begin{enumerate}
\item Ecologists cannot simulate their stats (or simple systems for that matter). Evolutionary biologists can. (And the field is better for it.)
\item Maybe hint at that you need these skills (and unit testing) given rise of AI?
\end{enumerate}


{\bf Take home messages (maybe)}
\begin{enumerate}
\item You should not fit a model you cannot simulate
\item Fit simpler models
\item Know your nonidentifiability
\end{enumerate}
\fi 

\newpage
\section{References}
\vspace{-5ex}
\bibliography{refs/bayesrefsmini.bib}

\clearpage

\section{Glossary}

% Words to define in table/glossary
% Classical approaches/methods. 

\begin{table}
\caption{A set of major terms used and simplified definitions.}
\begin{tabular}{ p{3 cm}  p{12 cm} }  \hline \hline
 \emph{Term}   & \emph{Definition}\\ 
\hline \hline
workflow & a set of steps to achieve a goal, with those steps designed to help organize the process, and ideally make it more systematic  \\\hline
statistical model & Approximations of observed phenomena via a mathematical equation, with parameters estimated from data using some fitting method. In this article, we often simplify to `model.' See also the Supplement: What's a model? \\\hline
calibration & analyzing how often an estimate is close to the true value; this requires knowing the true value, which we don't for our data---but we can calibrate models we plan to fit to our data (\emph{Steps 1-2}) so we understand the models better, including their limits given data similar to ours. We emphasize simulations to calibrate model behaviors consistent with our ecological systems and understanding (e.g., working within a limited set of parameter ranges through prior predictive checks). In contrast to this approach, frequentist method are calibrated against all possible behaviors, which is not only impractical in most circumstances itâ€™s also irrelevant given that the most extreme behaviors are unlikely to manifest in reality. \\\hline
prior & an distribution of reasonable values for a parameter based on fundamental biological and ecological understanding, previous research, or other sources \\\hline
posterior & product of the likelihood and prior (usually after a sampling methods such as MCMC) \\\hline
identifiability & when all parameters in a model can be uniquely identified with infinite data \\\hline
degeneracy & complex uncertainties that come from a mix of sources, including, non-identified models and cases where the data cannot well inform model parameters. When the data are not informing the parameters that we care about, this highlights a measurement issue. Identifying these problems in simulation studies, can highlight when we need a better experimental design (e.g., sampling for more overlapping species across sites, or changing what we measure, etc.).  \\\hline
\hline
\end{tabular}
\label{tab:glossary}
\end{table}


\newpage
\section{Figures (need work)}

\begin{figure}[ht]
\centering
\noindent \includegraphics[width=1\textwidth]{figures/workflow.png}
\caption{A very basic workflow for Bayesian model fitting include four major steps with potential feedbacks (pink dashed arrows) and begins with testing your model through simulated (test) data.}
\label{fig:workflow}
\end{figure}

\begin{figure}[ht]
\centering
\noindent \includegraphics[width=0.8\textwidth]{examples/synchrony/graphs/rawvsonepredictivecheck.pdf}
\caption{Example of a posterior retrodictive check from time-series data of phenological events over time (raw data on top; one simulated dataset on the bottom, based on existing species number and their respective $x$ data).}
\label{fig:retrodictivecheck}
\end{figure}

\begin{figure}[ht]
\centering
\noindent \includegraphics[width=0.5\textwidth]{examples/synchrony/graphs/retroSDsync.pdf}
\caption{Example of a retrodictive check from time-series data of phenological events over time, averaging across 1000 simulations.}
\label{fig:retrodictivecheckSD}
\end{figure}

\end{document}

