\documentclass[11pt]{article}
\usepackage[top=1.00in, bottom=1.0in, left=1in, right=1in]{geometry}
\renewcommand{\baselinestretch}{1.1}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{gensymb}
\usepackage{parskip}
\usepackage{xcolor}

\def\labelitemi{--}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead[LO]{}
\fancyhead[RO]{}

\begin{document}
\bibliographystyle{/Users/Lizzie/Documents/EndnoteRelated/Bibtex/styles/besjournals}
\renewcommand{\refname}{\CHead{}}

%%%%%%%
%% To do %%
%%%%%%%

% Test data code for y = ax + b?
% Ask WDP to add prior rants
% https://github.com/lizzieinvancouver/gelmanhill/wiki/Vocabulary
% TODO below


\title{How to fit Bayesian models and influence people \emph{or}\\
How to do Bayesian model fitting in ecology \emph{or}\\
The best way to be a Bayesian in ecology today}
\date{\today}
\author{EM Wolkovich and WD Pearse and TJ Davies and M Betancourt?}
\maketitle

% 160 words; main text is 4,000
\abstract{Improvements in algorithms and computational speed have heralded a new era of Bayesian model fitting. Models are easier to fit, faster to run, and more flexible, making them ideal for many of the complexities of ecological sampling designs. This has opened up Bayesian approaches to a new world of users. Yet many ecologists are not taking advantage of these new possibilities, and instead rely on similar approaches, work-arounds, and diagnostics that served them well for frequentist methods. Here we present a Bayesian workflow---centered on simulating data from models---that represents a ground shift of not just how to fit Bayesian models, but how we should approach statistical model fitting to advance our science. This workflow integrates mechanistic and statistical models with a computational toolkit, which we argue would accelerate ecological science. By interrogating our methods through generative models and data from such models, we can refine where to put resources for better estimates, better models, and better forecasts. While we outline these methods from the happy surf of our Bayesian beach, the ocean we describe works for anyone fitting models to ecological data.
% Steps of this workflow will often highlight current limits of our inference and suggest paths forward, as they provide a clear method to test how uncertain models are given small sample sizes, or how often a poor model may appear `best' given certain diagnostics. 
% Maybe along the way we highlight common practices in ecology that are stupid as all tomorrow---as our approach makes clear---and outline best practices for jumping into the wonderful surf of the happy Bayesian ocean. %JD16Aug: Obvious bad practices: small sample sizes and forking paths, thresh-holding on p-values, model selection and minimum adequate models etc.

\section{Main text}
\setlength{\parindent}{0pt}
\setlength{\parskip}{7pt}


Recent years have seen the explosion of Bayesian models across fields \citep{vandeschoot2021,schad2021,grinsztajn2021}, including ecology. This change comes in part from increased computational power, but more from new algorithms \citep[e.g. Hamiltonian Monte Carlo,][]{nuts2014,betan2019} that have made Bayesian models faster and arguably easier to fit \citep{Carpenter:2017stan}. Capitalizing on these advances \textsf{R} packages, such as \textsf{brms}, have seen much wider use than the previous generation of packages that streamlined fitting a set of pre-determined models using Bayesian approaches (e.g. \textsf{MCMCglmm}). Bayesian approaches, long heralded as more powerful, flexible and especially adept at capturing the multiple levels of variance in ecological data, seem poised to help ecology as a discipline advance in leaps and bounds, and to avoid contributing to the replication crisis.

Ecology, now three decades on after the start of the synthesis movement, has greatly advanced its statistical and computational approaches to test fundamental theory across systems \citep{Hampton2013}, but as ecological data has grown in size, so has its complexity. Bigger data is messier data. To make meaningful inferences from such data generally requires a model of not just the underlying biological processes, but also how the measurements were taken, what interruptions or various `demonic intrusions' \citep{Hurlbert:1984br}.  
% START HERE ... open _dothis_bayesianflows.txt and work in Mike's comments
% Classical methods are based on simple model assumptions, which are appropriate only when that data generating process is very clean.  One reason why the classical methods are becoming more fragile is that contemporary data generation processes are becoming dirtier.  The only way to do better is to spend way more time on cleaner experiments that are applicable to the classical methods or develop new methods bespoke to these more complicated data generating processes.  Of course Bayesian methods are particularly well-suited to the latter.
% Early concerns have given way to a widespread appreciation of the value of these new approaches.
%  (alongside a resounding and bizarre fervor over `natural history'). EMW -- commenting out my rant... and now the whole sentence. 


Ecology itself also seems poised to apply a century of theory and empirical insights to address the global challenges society is asking of it. Policy-makers demand predictive models of how communities and ecosystems will shift with climate change, habitat degradation, and other anthropogenic forces to balance conflicting societal needs and desires. At the same time, growing anthropogenic challenges have increased the need for predictive models and forecasts from ecologists. Policy-makers are faced with so-called `wicked' problems that cannot be simply solved, and so they need forecasts that communicate uncertainty so that they can communicate the inherent risks and trade-offs in decision-making.
% WDP: The above makes me think that we do need to get the whole marginal probabilities stuff into the text below
% [JD adds: models that embrace uncertainty, something we still find hard to communicate well, and variability vs mean responses...]

The result is that the average ecologist today has a diversified skillset compared to that of decades ago. Ecology has long been a field with deep statistical training, but the modern ecologist is now also expected to be computational: to handle large datasets, produce repeatable workflows, and translate models into forecasts for policy and planning. Many ecologists now bridge field, lab and computational methods. Bayesian methods can help facilitate this new job description. % As such, the rise of Bayesian approaches is especially timely for ecology. % The average modern ecologist is computational, but often grounded in the natural history of a particular system (err, maybe, on both these accounts, but whatever).

Bayesian approaches are not new to ecology (indeed they are over twice as old as the field of ecology itself), and they have long been used in certain subdisciplines related to estimating population sizes of things people want to eat or manage. For example, wildlife biologists frequently use mark-recapture data and associated models to estimate population sizes; such methods almost always use Hidden Markov models, which can rarely be fit without Bayesian methods \citep{muthuku2008,zheng2007}. Similarly, fisheries biology has tended towards more complex models---such as state-space models---to estimate stocks, which are similarly easiest to fit with Bayesian approaches \citep{trijoulet2018,millar2000}. % For decades, Bayesian training in ecology has focused on these aims, but as data and methods change, so has how ecologists are using Bayesian models. 

However, away from these fields, ecologists have largely focussed on frequentist statistics and null hypothesis testing. It's easy to see why: they are analytically straightforward (high-schoolers can carry out t-tests and ANOVAs armed only with pen, paper, and patience) and centuries of effort have led to standardized ways to address many questions. Such one-size-fits-all approaches can lead underlying assumptions on the presumed distribution of data to be ignored, spatial and phylogenetic data introduce non-independencies, and, in large ecological datasets, we often fit multi-way interaction terms that make main effects hard to interpret, or use random effects to correct for group level factors. Most concerning, as datasets become larger null hypotheses tend to be rejected even when the underlying effect sizes are weak \citep{gelmanhill,muff2022rewriting}. The modern frequentist statistician is now well-versed in the dangers and mitigations for such issues, but arguments for simplicity start to become weak.

Today, as large-scale ecological data becomes available for more diverse systems and for questions addressing other aims, ecologists are using Bayesian models in new ways. Yet, there is naturally a lag in training in statistics, and in Bayesian modeling in particular. Bayesian approaches provide a pathway to powerful models that can transform how we understand our systems, but they can also lead to pitfalls most ecologists are not trained to notice or deal with. Many of these pitfalls can be avoided by approaching Bayesian analyses through specific workflows \citep{betanworkflow,vandeschoot2021}, which themselves are built on a process of how to do not just stats, but how to do science. 

Here we describe a broadly generalizable workflow for Bayesian analysis and show how it can revolutionize training in ecology by integrating more model building and model understanding. Once you start doing this workflow, your scientific life will never be the same. 
%  allow fitting complex models without much influence of the data fed into the model. 

\subsection{A bluffer's guide to what Bayesian statistics is}
To better understand our workflow, we provide a very brief overview (thus inherently incomplete and by design not very technical) of some of the fundamentals of Bayesian methods. This section can be skipped for those who feel already well-versed, and can be augmented for those who are new to Bayesian approaches \citep[see, for example,][]{statrethink,BDA,regotherstories}.

Many of us have some intuition as to what a probability is, and often it is ``the long-term frequency with which something happens.'' We would expect, if we tossed a coin 100 times, that we would get roughly 50 heads: we would say that the probability of tossing a coin and getting a `head' is $\frac{50}{100}$, which is 50\% or $\frac{1}{2}$. We wouldn't be very surprised if we got 49 or 55 heads, but we would be surprised if we got 99.

This definition of probability---which is the \emph{frequentist} definition---is useful in many situations, but it has a few disadvantages. First, frequentist definitions aren't very useful when dealing with totally unexpected situations. Frequentist probabilities are grounded in \emph{data}; thus, if we don't have a lot of data we struggle to generate estimates of our uncertainty. We don't, for example, have an idea what the probability of life on Mars is because we've only looked at one planet, and so we don't have a long-term frequency with which, when we're exploring planets, we find life. Similarly, graduate students are often sent out into places where no one has worked before, which can make it difficult to figure out whether what they see in their first year of data collection is unusual, or to make predictions for their second year. Thus it would be nice to have a statistical way that can \emph{propagate our (un)certainties about we do(n't) know} into our statistical modelling.

Luckily there is an alternative definition of probability that allows us to incorporate so-called \emph{belief} into our definition of probability: Bayesian probability. The Bayesian definition of probability is:

\begin{equation}
  probability = \frac{likelihood \cdot prior}{normaliser}
  \label{bayes_theorem}
\end{equation}

Where $probability$ is the Bayesian probability, $likelihood$ is exactly the same as a frequentist likelihood, \texttt{prior} is your best-guess of the probability (your `belief prior' to collecting data), and \texttt{normaliser} is a mathematical constant that makes sure our probability cannot go above 100\% or below 0\% (statisticians are lazy, and will not `give 110\%'). This mathematical constant (technically it is the probability of our data; $p(data)$) is a nuisance term that is extremely challenging to estimate (sometimes it is impossible!) and held back the practical application of Bayesian statistics for many years. Nowadays, with increases in computer power, we can use numerical methods such as `Markov Chain Monte Carlo' (MCMC) methods to avoid having to estimate its value precisely. Such methods are simulations  that involve chance at each step (that the process proceeds in iterations or steps, and only the last iteration affects the next, is why this is a `Markov Chain'), tweaking and changing coefficients within your model until a distribution of plausible coefficients are found. This distribution is called the `posterior' distribution to distinguish how it is our view of probability \emph{after} the data and the prior have been considered. There is no guarantee that the posterior distribution will, itself, be found: the user must check for so-called `convergence' onto the true posterior distribution as the process involves randomness. It is for this randomness that `Monte Carlo' is added into the name, after a city famous for gambling.

It turns out there is a further advantage that such an approach gives us. While frequentist probabilities are all about the probabilities of observing data ($p(data|model)$: what are the chances of getting 99 heads in 100 coin-tosses?), Bayesian probabilities are all about the probabilities of observing models ($p(model|data)$: what are the chances this coin is biased towards heads?). This makes it straightforward to report the results of Bayesian analyses in terms of estimates of uncertainty and precision of coefficients (contrast ``I'm 80\% certain that coin comes up heads at least two-thirds of the time'' with ``I'm less than 5\% certain those last twenty coin tosses came from a fair coin'').

% WDP: I started trying to write about how you can do conditional probabilities using posterior distributions but I realised that I'm beginning to stray into the benefits and pitfalls section below. 

\subsection{A brief overview of the benefits---and pitfalls---of Bayesian models} 

Bayesian models have many benefits, but an often-mentioned one is that `you can fit any model you want.' While this is not entirely true \citep{BDA,reid2019}, Bayesian modeling options can feel limitless when compared to the models ecologists can fit in popular modeling packages (e.g. \textsf{lme4}). As long as you can write out the likelihood of your desired model and assign priors to all parameters, you can generally `fit' any model. This includes non-linear ones, non-Gaussian families (e.g. Poisson, beta or combinations thereof, such as hurdle models), hierarchical designs and any combination of these, as well as `joint' models where parameters estimated in one equation appear in another, carrying along estimated uncertainty. Such flexibility is incredibly powerful in ecology where data are often influenced by complex spatial or temporal patterns, non-linear processes are widespread, and common data types are non-Gaussian (e.g. counts, percent cover etc.). 

Fitting a bespoke model to data also yields numbers we often really want but don't have access to in other approaches. We perform experiments because we want to know how a treatment affects something of interest---how much slower to do birds wearing backpacks fly, or how much faster to plants grow with more warmth---but we often become more focused on whether the treatment was `significant' or not. We lose track of whether birds flew 15\% or 50\% slower, and how consistent the effect was across individuals. But models can be designed to estimate and report effects per mg of backpack weight, or per \degree C of warming---always with estimated uncertainty. While replication crises in other fields, driven in part by a overly zealous focus on $p$-values \citep{halsey2015,loken2017}, and the rise of meta-analyses in ecology \citep{Hampton2013} have led to a somewhat greater focus on `effect sizes' in ecology (often used to refer to very specific unitless statistics, such as Cohen's $d$), bespoke models take this to a new level. Researchers can easily estimate comparable effect sizes from $z$-scored data (in units of standard deviation), alongside estimates in meaningful natural units, such as per \degree C of warming or per hectare of habitat lost.

% WDP: Below is a point that I think follows from what you have above; it's also something I'm keen on, as I think I've banged on about by now. Apologies because this text is a little rough; hopefully the spirit is coming through though.
Further, using Bayesian methods makes it easy to report estimates of probabilities about coefficients (\emph{e.g.}, ``with anthropogenic warming, we are 80\% certain the flowers open earlier'') rather than null models (\emph{e.g.}, ``we are less than 5\% certain these data came from a model where flowers aren't responding' to anthropogenic warming'). Relatedly, frequentist tests are based on accepted level of Type I error rates---a particular $\alpha_{crit}$ (often 5\%), which is often confused with statistical power ($\beta$, the probability that we would detect a true effect): when we define significance as a less than 5\% chance that some data came from a null hypothesis, it does not mean we have a 95\% chance of detecting a true effect \citep[indeed statistical power is often extremely low in ecological studies,][]{jennions2003survey}. Bayesian statistics, by allowing us to compare the probabilities that \emph{models} are correct, and not the probabilities that \emph{data} are taken from a give model, allows us to test what we have always cared about: the bioological processes themselves.

This valuable flexibility is often mentioned as one of the greatest pitfalls of Bayesian models: you can fit almost whatever you want, but critical parts of your model might be almost entirely unimpacted by your data. In ecological model fitting, we're currently most often interested in parameter estimates strongly informed by our data, making this problem sound especially dangerous. In reality, however, such models are not as common as some may suggest \colorbox{pink}{(CITEs from Will?)}. Perhaps more dangerous is fitting mis-specified models, where the model is not doing exactly what we think, either due to coding errors or poor understanding of the model. 

These `pitfalls' of Bayesian are not new, not necessarily specific to Bayesian methods \citep{low2014rising}, nor unique to ecology---though the complexity of ecological data and processes may make it especially pernicious in ecology---and decades of statistical research has aimed to develop best practices when using Bayesian models to avoid this. These best practices generally center around a specific workflow; a variety of which can be found in exquisite detail elsewhere \citep{betanworkflow,grinsztajn2021,vandeschoot2021}. We do not aim to repeat those here, but instead to provide a highly simplified---but powerful---workflow that, when applied to Bayesian modeling in ecology, could greatly accelerate progress. % This is a short overview of a much deeper topic. See \href{https://www.nature.com/articles/s43586-020-00001-2}{Bayesian statistics and modelling} for a little more depth, including how to develop priors and your basic model formulation.\\

\subsection{A very basic Bayesian workflow}

We outline a Bayesian workflow below that includes what we consider the major steps for Bayesian model fitting (Fig. \ref{fig:workflow}). Many steps should be familiar to statistical ecologists, but are often overlooked, whereas other steps may appear particular to Bayesian (e.g. prior predictive checks), but are actually useful for anyone---using Bayesian models or not---to challenge their models of how the world works. 

As we focus on a simplified list of major steps, many of the smaller but still critical steps are omitted. For example, visualization is required at every step---especially 2 and 4; while we do not discuss this explicitly we refer to relevant publications. We also jump in, and somewhat over, one of the biggest steps. 

We present the workflow assuming users have a model already in hand, which---if you have collected data---is true. Your model may be only verbal or conceptual; however, for this workflow, you'll need to convert such models into mathematical versions. This can be challenging at first, as ecologists often learn only a simplified version of this step (often focused on identifying which distribution---normal, binomial, zero-inflated Poisson, etc.---their response variable most closely resembles), and approach it after data collection. In reality, skilled users of this workflow will develop their model before they collect and data refine it---or develop possible additions to it---as they collect data; models often fit best when they include both the underlying biological model being studied and a model of the data generating process (e.g., gaps between sampling dates, biases in sampling etc.). 

Getting to the point where the Bayesian workflow is part of data design and collection, however, requires starting somewhere---with some model in hand. A suite of resources on generative modeling can help \citep{statrethink,betangen}, along with two points. First, you must start somewhere, so know that you can and will improve on this skill. Second, as you start, ask lots of questions---and push yourself on your answers---what do you expect and what's reasonable biologically from your model? For example, instead of simply identifying to which distribution your response variable looks most similar, ask yourself what generates that distribution and what you think its mean, minimum and maximum is. Do you expect data below zero? Up to infinity? If not, why not? You'll be generating your model---including its priors---as you do this. 

Our workflow is explained mostly program-agnostically. Though at times we assume a user of \textsf{Stan}, a relatively new probabalistic programming language, that interfaces with \textsf{R, Python, Julia} (and more) to write bespoke Bayesian models and underpins the \textsf{R} packages \textsf{brms} and \textsf{rstanarm}, which fit a suite of specific (pre-defined) models \citep{Carpenter:2017stan}. We focus on \textsf{Stan} as its MCMC algorithm (a variant of Hamiltonian Monte Carlo, HMC) is fast and produces specific output to warn of model fit issues (i.e., divergent transitions) in a way other MCMC algorithms do not (e.g. Metropolis-Hastings or GIBBS), but the basic workflow should apply to diverse implementations of Bayesian modeling. \\

\emph{Step 1: Check your model code.} \\ 
% This obvious step assumes a suite of crucial work already done to define your model of interest. You must understand your question, your subdiscipline, and your data enough to formulate a useful model that you want to fit. This step thus assumes you arrive with a wealth of scientific expertise and a well thought-out model. 
To start the workflow, you need to write up your model and check it. As with all code, just because it runs, does not mean it does what you think it does. Whether writing it out in \textsf{Stan}, where you need to be able to write out the full likelihood and set all your own priors, or using a package that writes much of the model for you (e.g. \textsf{rstanarm}), you need the code and a way to verify the code is correct---test data.

Test data (aka `simulated data', or  `fake data,' etc.), and the skills required to build it, are central to this workflow. The idea of `test data' is that you simulate data from your model in such a way that you can use the resulting data to test your model code is correct. This means that to build test data you need to understand your model well enough to generate data from known parameters (which you select); you then run that data through your model and confirm it returns the parameters. %  As the goal is to test your model code, you'll want test data that is well-replicated, balanced and otherwise ... (err, actually -- skip this -- as it assumes a model without a data generating process in some ways). 

This very basic model checking step is uncommon for many ecologists, but critical in our view. If you can simulate data from your model, then you can powerfully---and easily---answer questions related to statistical power, what effect sizes are reasonable, and---most likely---have new insights into how your model suggests the world works. `All models are wrong; some models are useful,' becomes much clearer when you have the power to generate data from your model under any parameter set you want. Conversely, if cannot complete this step, you'll struggle to understand if the model fit well, and struggle further to meaningfully interpret the model output, making this apparently simple programmatic step actually encapsulate a far deeper understanding of your model. Once your \textsf{Stan} output returns the parameters you expect from your test data, you can move onto interrogating your priors. 

\emph{Step 2: Check your priors.} \\
Priors are the source of much discussion and focus by Bayesian and non-Bayesian researchers alike. Often treated as the big bad wolf of Bayesian, or the unseen hand producing the model fits you get, according to some. They show up as half of the equation that gives you your model posterior, and philosophically, Bayesian was built around the idea that you have prior knowledge that you trust and want to compare to new data (with your new data showing up through your likelihood). In reality, few Bayesian analyses in ecology are approached this way. 
%JD16Aug --  Dolph always compares the 'danger' of priors in the belief that Saddam Hussein had WMDs - data was sparse, so priors overwhelmed. Han Solo flying the Millennium Falcon through an asteroid field illustrates the benefits of priors. We need to know where we are on the Saddam - Han spectrum.
% WDP: Jonathan, this is the first time I've read you making a sci-fi reference in print and I'm extremely excited by it

How much priors influence your model fit is up to your model and your data. Depending on those two parts, the likelihood (influenced by your data) can easily overwhelm your priors. Indeed, most work on the dangers of priors and `prior misspecification'  focuses on cases where you have very little data for the model you're trying to fit \colorbox{pink}{[Refer to your rant paper here?]}. If you try to fit a Bayesian model with 100 parameters and only 5 datapoints, then your priors will likely matter a lot. Priors can also matter when you have fewer parameters and lots of data but the data are not informative for the model; for example, if you fit a model including parameters for estimating high temperature responses, but lack data at those temperatures. Priors, however, can only matter more than you know when you fail to think through and check them. 
% In our experience, however, the most dangerous part of priors is not that they will matter more than you know, it's that in failing to think through them and check them, then you'll fail to understand your model enough, and be unprepared to interpret its output. 

Assigning priors generally forces you think about your model with regards to your study system, and interrogate what's probable, possible or actually crazy. While many packages (e.g., \textsf{brms, rstanarm}) will set priors for you \colorbox{pink}{[could add ref to rant here ]}, assigning them yourself (which you have to do if you write your own code) can quickly disabuse newbies to Bayesian and skeptics of their prejudices. For example, you may not think you have a prior on how sunlight affects plant growth, until you realize your `agnostic prior' actually allows plants to grow hundreds of meters per day. % something like `and generally this will not impact your results, see Supp'

You can take this a step up with prior predictive checks, which serve both as a check on the priors you're using, and to further explore the model of the world to which you're planning to fit your data. In prior predictive checks (which can easily be done by adapting your code from Step 1), you explore a distribution of potentially reasonable values for your priors, then see how they influence your resulting output. How exactly to do this depends on your question, model and aims, but many guides can help you think through this \citep{betanprior,wesner2021,winter2023}. 

By looking at a full distribution of priors you may suddenly realize that prior values that previously seemed reasonable lead to heavily unrealistic results when embedded in your full model, with all the other priors on parameters. You may find you have set up a model where certain effect sizes mean birds fly backwards when given heavy-enough backpacks. This means both that you may want to adjust your priors, but also gives important insight into what certain parameter values lead to. Thus, if you see them in your actual model output you'll be more likely to realize your model is problematic (Step 4 will also help with this).

 \emph{Step 3: Run your model on your empirical data.} \\
The next step is to run the model---you've now validated, test-run and have ready to go---on your exciting new empirical data. Check diagnostics so you know it's running well and adjust until it is; this includes a suite of convergence and efficiency metrics ($\hat{R}$, ESS, lack of divergent transitions etc.) that are well-discussed elsewhere \citep[and not our focus here, see instead][]{betanworkflow,gelman2020bayesian,vandeschoot2021,gabryvis}
 
This is the step many ecologists skip straight to, ourselves included. It's easy to see the appeal---fitting our new data to the model can feel like the moment when we'll learn something new. But, at least in our experience, this is not always the case. When we rush to this step, that first model we fit is often followed by another, and another---perhaps because one does not converge, or the results of another do not make immediate sense. After a while of this process, it can easily feel like we're not sure what we learned, if anything. And we can get distracted from what we are actually most interested in---the biological effects. In contrast, by approaching the model through Steps 1-2, it's often much easier to quickly see through the results of the model fit. And also easier to plan next steps. 
 
% This is the step many ecologists skip straight to, leaping over steps 1-2 without realizing all they are missing along the way. While we can understand this approach---given the excitement of wanting to see how new data influences the model---we hope it's now obvious how much more can be gleaned from data by approaching the model through Steps 1-2. 
%  As we're all familiar with this step, it's hopefully straightforward. Run the model---you've now validated, test-run and have ready to go---on your exciting new empirical data. Check diagnostics so you know it's running well (convergence metrics, lack of divergent transitions etc.) and tweak until it is. 

\emph{Step 4: Posterior retrodictive checks} \\
Once you have your posterior based on your model and new empirical data, it's time to interrogate it. Just because your model gave you estimates doesn't mean the model is adequate for the data, and---depending on why you fit the model---you may especially want to make sure it is reasonably predictive. You can do some of this through diagnostics, such as $R^2$ and examining posteriors, but simulating from your model can be especially insightful. Steps 1-2 have set you up well for this, as you have a sense of what different parameter estimates do to the model.

Now that you have the parameter estimates from your posterior you can simulate new data from them and see how that new world looks to you---called posterior retrodictive checks (or posterior predictive checks, Fig. \ref{fig:retrodictivecheck}-\ref{fig:retrodictivecheckSD}). Exactly how to do these are---again---dependent on your question, model and aims \citep[but there is lots written on this,][]{held2010,gelman200ppc,conn2018}. In contrast to prior predictive checks, however, this step is built into some software. If you use \textsf{rstanarm}, then the package \textsf{shinystan} will automatically give you a set of posterior retrodictive checks, including comparisons of the mean and variance of simulated datasets with those of from your empirical data. 

Often here you may find big differences from your empirical data, and can start to generate hypotheses for why. For example, you may find patterns that suggest missing grouping factors (e.g., site or biome) through visualization, and by grouping posteriors by that factor, or you may quickly realize your model predicts impossible numbers for your biological reality because of the distribution. 

\emph{Feedbacks in the workflow}\\
With those hypotheses in hand, you may very well want to tweak your model---this is all part of the workflow. At that point you return to Step 1, tweak your code, and repeat the process. In this way, fitting multiple models is encouraged, but in a far more structured and careful way than traditional ecological model fitting in our experience. This approach is different than the quest for a minimum adequate model or one `best' fit. Feedbacks in this workflow are focused far more on what is biologically reasonable, and understanding the utility---and limits---of inference from your data for your model. And there are big benefits to it. 

This process more fully integrates mathematical modeling into statistical modeling. To complete Step 1, you have to understand the underlying math of your model enough to simulate data from it. This can be challenging at first (e.g. in our courses many students cannot recall how to simulate $y$ data for a simple linear regression), but is immensely beneficial to understanding your model. Indeed, we have found the greatest insights come not from the step we all know best---fitting the model with empirical data---but from every other step in this workflow. Developing simulated data to test the model, running prior and retrodictive checks all dive you deep into understanding your statistical model, which suddenly you may find yourself thinking through much more mechanistically. In our experience this process has quickly translated into insights for our biological systems, and changed how we approach statistical models. 

\subsection{How this workflow changed our science} % Surprising things that happened to us that may happen to you if you follow this workflow
% JD16Aug -- I wasn't sure I was going to like this section, but you won me over - I just struggled with the 'simplify' section a little 

As we have used this workflow, how we approach our statistical models has changed. These changes have generally been similar for each of us. We suspect they are not unique to us, our study systems or questions. Instead, we think they represent common approaches to statistical modeling in ecology that could help the field advance, much as we believe they have helped our science advance. 

 \emph{Understanding nonidentifiability} \\
Identifiability and---more common in our experience: nonidentifiability---refers to when all parameters in a model can be uniquely identified. Models can be nonidentifiable in several ways, including when mathematically some parameters cannot be uniquely defined. Statistically, nonidentifiability often is an outcome of the empirical data combined with the model. In this definition nonidentifiability occurs when the data do not contain ``sufficient information for unique estimation of a given parameter or set of parameters in a particular model'' \citep{gelmanhill}. 

Nonidentifiability can come up in many ways in ecology---and be hard to see, especially if you rush through model fitting. But if you have to write out your model and simulate data, you may suddenly realize lots of places for nonidentifiability to live. For example, when species do not occur across most sites, a model including separate parameters for site and species is often nonidentifiable, but there's no warning in packages to tell you this; you have to learn it from the data. We have become far better experts at nonidentifiability based on this workflow---and we have adjusted how we collect data and interpret results because of it. % JD16Aug -- and sometimes it doesn't matter if your are interested in the effect of a separate parameter.

 \emph{Know your limits} \\ % Dive into the simplified complexity conundrum
Once we noticed how pervasive nonidentifiability and weak identifiability (where parameter estimates are highly uncertain because the model and data together are nearly nonidentifiable) were we started simplifying our models. Whereas previously when we had data that qualitatively appeared complexly nested, crossed, split or twisted, we would have tried to fit all of these intricacies (on the intercept) we are now more slow to add these to our models. By both understanding these terms better (including the many different ways each can be modeled), and understanding them better---depending on the data and model in each unique context---we work more carefully through what to include. % JD16Aug -- I feel there is a tension between the philosophy of 'if you think it might be important, then include it in your model' and make your model simple. We want both. But perhaps all those crazy interaction terms that are impossible to make sense of could be ignored ... 

% JD16Aug -- this was even referred to as 'model simplification' and we searched for MAMs (minimum adequate models) - See Crawley.
Before using this workflow, some of us would start with complex models, then simplify models until they converged. Often these were hierarchical models with many levels, built based more on a dogma of non-independence (and, somewhat relatedly, correct degrees of freedom) than an understanding of the model. We also often fit a suite of interactions: multiple two-way interactions and the occasional three-way interaction were common fare. But in simulating data, and fitting models to real, messy, imbalanced data using the workflow we came to see how much we were asking of our data and models together. Fitting a two-way interaction with half the effect size of a main effect takes a 16X sample size, compared to fitting the main effects alone \citep[the main effects then average over the interactions, see][for more details]{regotherstories}. This is sobering. It's more sobering when you see it played out again and again through this workflow. 

We now both add complexity and simplify based on a more careful reckoning. Usually our starting model is not simple, it often includes grouping factors that may be difficult to fit, but that we see as absolutely critical to the question, model and data at hand. We still may add and consider additional grouping factors and interactions, but we do so with a careful idea of how stable the model likely is with them, and we rarely fit complex three-way interactions or similar---unless we have carefully designed the model and data collection for that aim. The ending model is often not as complex as we may have fit if we did not follow all the steps. And---through them, especially Step 4---we can feel confident our model is capturing important variation. 
% Once you do the workflow, you may end up like us: fitting fewer levels in your mixed effects models, fitting fewer interactions.

Understanding this variation includes re-approaching how we plot our model and data. While many of us are good at plotting raw data, and plotting our main model parameters (and plotting both these pieces together), the world in between is rarely taught. And it's in this in-between world that we have found a deep understanding of our models, what they're doing and---relatedly---what our results show. Many packages offer model predictions---based on a full or near-full model, but being able to decompose model predictions into varying components can give powerful insights. For example, model predictions can be plotted with and without major grouping factors such as `block,' `site', or `species,' and suddenly how predictable a model is (or not) based on other factors like `treatment' or `time' becomes clear. 

 \emph{Looking at parameters, not p-values} \\
Before this workflow, not all of us commonly discussed the values that parameters in our model took---things like the slope and intercept (two common model parameters) were sometimes reported, but we did not know them as well as we knew whether the $p$-value for the slope was $<0.05$. This changes quickly when you need to build simulated data, for example, for a phenological event (a day within the calendar year so, ideally between 1 and 366) and suddenly find it's $>$1000 based on the quick parameters you put into your code. 

This focus on the value of parameters scales up through the workflow and across projects. Having a better sense of parameter values across different biological contexts, model parameterizations, and time periods gives a better sense of how the biological world works, including what's reasonable, possible or wildly unrealistic. % Suddenly, weakly informative priors make ever more sense .... 

\subsection{How this workflow intersects with ecological training} %Challenges of the workflow compared to how we traditionally train ecologists

This four-step workflow is a simplified version of the current best practices for Bayesian model fitting  \citep{betanworkflow,vandeschoot2021}, but many of the skills required are not part of traditional ecological training. Writing out the math behind most statistical models enough to complete Step 1 bleeds into the skillset usually reserved for those working on theory, where coding and simulating from a model are common tasks. In contrast field, lab and otherwise empirical-data based ecologists often fit models they could not simulate data from. This dichotomy seems short-sighted in our current era, of bigger, messier data and greater computational methods poised to handle such messy data---if scientists are trained in how to build the right models for their questions and data. A reasonably competent coder could easily simulate data under a complex model that they might not have the mathematical expertise to solve. % We believe training ecologists with a skillset where they quickly can use this workflow could advance ecological science rapidly. % Theory = simpler models = outcome of a good Bayesian workflow (often)

This workflow also requires a shift in how we approach Bayesian methods---and training in Bayesian models---in ecology. Already, uptake of new Bayesian \textsf{R} packages highlight that Bayesian methods are no longer the purview of only wildlife and fisheries biologists, and these changes come alongside advances in Bayesian workflows, algorithms, and visualiziations \citep[e.g.][]{betanworkflow,vandeschoot2021,gabryvis}, that ecology must adapt its training for. While this is an active area, we highlight three major changes.

(1) Prior `beliefs' are changing. Best practices for determining priors is an active area of statistical research \citep{BDA,regotherstories}, and training should reflect current best practices. These include that `non-informative priors' are a misnomer, as they are often informative  \citep{lemoine2019}, and priors can easily be `weakly informative.' Thus a strong focus on the dangers of priors in training can be overkill \colorbox{pink}{[Ref to rant?]}. 

Current training often includes a very strong focus on priors, including conjugate priors, because of the importance of conjugate priors in closed form solutions for particular posteriors. Modern algorithms, such as HMC, do not require conjugate priors, which are now antiquated (and thus sometimes referred to as  `the crystal deodorant of Bayesian statistics'). Prior predictive checks provide a far more powerful way to understand how priors work within a particular model, and are far more useful than rules about which priors should be fit in certain cases or memorizing which priors are conjugate. 

(2) `Random effects' are not just random. Hierarchical models contain grouping factors, sometimes referred to as `random effects,' such as species or individual. This term, however, is misleading, imprecise and thus no longer recommended \citep{gelmanhill}. In ecology, it also carries with a heavy weight of older `rules' of what is `random' versus `fixed,' including that `random effects are things you don't care about (for example the `block' effect from a randomized block design). After posterior retrodictive checks (Step 4), you might feel differently, as hierarchical effects are (by definition) drawn from an underlying distribution---meaning you can predict outside of the specific set you sampled (for example, you can predict for a new species or individual), whereas you cannot do so for most categorical `fixed effects.'  

(3) P-values are easily misleading (\href{https://www.youtube.com/watch?v=c3hxhv0lpI0}{`I am arbitrary but my story is often told ... '}), and there's no easy fix for that. The replication crisis, rampant in other fields, is based in part on an overly hopeful belief that $p$-values will separate the signal from the noise, with one easy number. In reality, small sample sizes, lack of routine reporting of interpretable effect sizes, fitting of many models without adequate explanation, poor data and code reporting habits all increase the chance of finding `significance' at a level of $\le0.05$ \citep{halsey2015,loken2017}. This reality means a similar crisis is likely lurking in ecology, especially given small sample sizes alongside a tendency to fit complicated models with multiple interactions. The answer to this, however, is not to make $p$-values smaller \citep{halsey2015,colquhoun2017}, nor is it Bayesian approaches, which (as we touch on above), bring their own ways to sift `exciting results' from what is actually a pile of chafe. 

The answer is a workflow for careful model building, model fitting and model interrogation---as we outline here. %Try it, and see if it transforms your life they way it transformed ours!
This workflow depends strongly on simulating data---for testing your model (Step 1), checking your priors (Step 2) and understanding your model results (Step 4)---an area we actively under-train in ecology. This workflow makes clear how relevant and important simulation is, but the relevance of simulation extends well beyond Bayesian model fitting. As larger datasets and machine learning increase their utility, and prevalence, methods to test our understanding, interrogate our models and develop new models, will depend strongly on simulation, potentially transforming ecology, as it is transforming science in general \citep{flynn2022digitaltwin,kuntz2022,oren2017}.

\iffalse
\subsection{Conclusions}
\begin{enumerate}
\item Ecologists cannot simulate their stats (or simple systems for that matter). Evolutionary biologists can. (And the field is better for it.)
\item Maybe hint at that you need these skills (and unit testing) given rise of AI?
\end{enumerate}
\fi 

{\bf Take home messages (maybe)}
\begin{enumerate}
\item You should not fit a model you cannot simulate
\item Fit simpler models
\item Know your nonidentifiability
\end{enumerate}

\newpage
\section{References}
\vspace{-5ex}
\bibliography{refs/bayesrefsmini.bib}


\newpage
\section{Figures}

\begin{figure}[ht]
\centering
\noindent \includegraphics[width=0.7\textwidth]{figures/workflow.png}
\caption{A very basic workflow for Bayesian model fitting include four major steps with potential feedbacks (pink dashed arrows) and begins with testing your model through simulated (test) data.}
\label{fig:workflow}
\end{figure}

\begin{figure}[ht]
\centering
\noindent \includegraphics[width=0.8\textwidth]{examples/synchrony/graphs/rawvsonepredictivecheck.pdf}
\caption{Example of a posterior retrodictive check from time-series data of phenological events over time (raw data on top; one simulated dataset on the bottom, based on existing species number and their respective $x$ data).}
\label{fig:retrodictivecheck}
\end{figure}

\begin{figure}[ht]
\centering
\noindent \includegraphics[width=0.5\textwidth]{examples/synchrony/graphs/retroSDsync.pdf}
\caption{Example of a retrodictive check from time-series data of phenological events over time, averaging across 1000 simulations.}
\label{fig:retrodictivecheckSD}
\end{figure}

\end{document}

